{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Objective\n",
        "The objective of this exercise is to observe the different representations of the same word occuring in different contexts."
      ],
      "metadata": {
        "id": "XKdefrTaQgG9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BERT\n",
        "BERT, or Bidirectional Encoder Representations from Transformers, is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.\n",
        "\n",
        "Meaning that a general-purpose \"language understanding\" model is trained on a large text corpus (like Wikipedia), and then use that model for downstream NLP tasks that we care about (like question answering, classification etc)."
      ],
      "metadata": {
        "id": "tXkJYV0expkL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NGI9StTjt54Y"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "pip install tqdm boto3 requests regex sentencepiece sacremoses transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# transformers\n",
        "PyTorch-Transformers is a library of state-of-the-art pre-trained models for Natural Language Processing (NLP).\n",
        "The library currently contains PyTorch implementations, pre-trained model weights, usage scripts and conversion utilities for the models like BERT, GPT, XLM, RoBERTa, BistilBERT\n",
        "\n",
        "[LINK FOR TRANSFORMERS](https://pytorch.org/hub/huggingface_pytorch-transformers/)\n",
        "\n",
        "# DistilBERT\n",
        "A smaller general-purpose language representation model which can then be fine-tuned with good performances on a wide range of tasks like its larger counterparts. \n",
        "\n",
        "[MORE ABOUT DistilBERT](https://huggingface.co/docs/transformers/model_doc/distilbert)\n",
        "\n",
        "This approach reduces the size of a BERT model by 40%, while retaining 97% of its language understanding capabilities and is 60% faster."
      ],
      "metadata": {
        "id": "xL_VpVm8v98A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DistilBertTokenizer, DistilBertModel\n",
        "import torch\n",
        "from sklearn.manifold import TSNE\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "SdQ41OrouDId"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Using the tokenizer provided to us \n",
        "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
        "model = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")"
      ],
      "metadata": {
        "id": "qSyPkOJLuOYk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_features_diff_context(sentence_list, word_of_interest_list):\n",
        "  \"\"\"\n",
        "  sentence_list: a list of sentence\n",
        "  word_of_interest_list: list of words which occur in the corresponding sentence, \n",
        "                          whose representation we are interested in\n",
        "  \n",
        "  return dict\n",
        "   key: word_s:{1,2,3...n } - word of interest and the index of the centre it occurs in.\n",
        "   value: \n",
        "  \"\"\"\n",
        "  assert len(sentence_list) == len(word_of_interest_list)\n",
        "  sentence_list = [sentence.lower() for sentence in sentence_list]\n",
        "  inputs = tokenizer(sentence_list, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "  outputs = model(**inputs)\n",
        "  reps = outputs['last_hidden_state']\n",
        "  out_dict = {}\n",
        "  # words_of_interest = ['good', 'good', 'good', 'good', 'excellent']\n",
        "  for i, tokens in enumerate(inputs['input_ids'].tolist()):\n",
        "    for tok_pos, tok_indx in enumerate(tokens):\n",
        "      tok = tokenizer.convert_ids_to_tokens(tok_indx)\n",
        "      if tok == word_of_interest_list[i]:\n",
        "        out_dict[f'{tok}_s:{i}'] = reps[i, tok_pos, :].detach().numpy()\n",
        "  return out_dict            \n"
      ],
      "metadata": {
        "id": "sWaugpaivAwV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence_list = [\"the river bank was quite nice\", \"The bank ran out of money\"]\n",
        "word_of_interest_list = [\"bank\", \"bank\"]\n",
        "\n",
        "bank_different_representations = get_features_diff_context(sentence_list, word_of_interest_list)\n",
        "\n",
        "print('The representations obtained from sentence_list and words_of_interest:')\n",
        "for k,v in bank_different_representations.items():\n",
        "  print(f'key: `{k}`, representation_dimensions {v.shape}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FQxMXvG54aAT",
        "outputId": "4d2bc94c-adda-4b5b-944b-9a8ba0714467"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The representations obtained from sentence_list and words_of_interest:\n",
            "key: `bank_s:0`, representation_dimensions (768,)\n",
            "key: `bank_s:1`, representation_dimensions (768,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise 1\n",
        "Similar to the above example:\n",
        "Get the representation of:\n",
        "\n",
        "a. `good` from the sentence: `that is quite good`\n",
        "\n",
        "b. `good` from `that is very good`\n",
        "\n",
        "c. `good` from `that can be good`\n",
        "\n",
        "d. `bad` from `that is bad`\n",
        "\n",
        "Store the result of `get_features_diff_context` in a variable named `word_feature_dict`\n"
      ],
      "metadata": {
        "id": "G1GZ18XG5v0m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## YOUR CODE GOES HERE\n",
        "\n"
      ],
      "metadata": {
        "id": "pUBR1KjuyPPc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Run this cell as it is:\n",
        "{key: value.shape for key, value in word_feature_dict.items()}"
      ],
      "metadata": {
        "id": "x70tfsba4H0w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "expected result:\n",
        "```bash\n",
        "{'good_s:0': (768,), 'good_s:1': (768,), 'good_s:2': (768,), 'bad_s:3': (768,)}\n",
        "```"
      ],
      "metadata": {
        "id": "3HNtCfVs88HR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise 2:\n",
        "Implement a similarity function that takes the previously generated set of key and features and calculates the cosine similarity of one representation with all other representation except itself.\n",
        "\n",
        "e.g. of output:\n",
        "\n",
        "```python\n",
        "{\n",
        "  'good_s:0 & good_s:1': COSINE_SIMILRITY_VALUE,\n",
        "  'good_s:0 & good_s:2': COSINE_SIMILRITY_VALUE,\n",
        "  'good_s:0 & bad_s:3': COSINE_SIMILRITY_VALUE,\n",
        "  .\n",
        "  .\n",
        "  .\n",
        "}\n",
        "\n",
        "```"
      ],
      "metadata": {
        "id": "7tSEPJ6g9OhC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def similarity(rep_dict):\n",
        "\n",
        "  # YOUR CODE GOES HERE\n",
        "  pass\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "qYiT5n9TBaGe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "similarity_dict = similarity(word_feature_dict)\n",
        "for k,v in similarity_dict.items():\n",
        "  print(f'{k}:{v}')"
      ],
      "metadata": {
        "id": "Sfu2_3dSlvt3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise 3\n",
        "1. In the results of `Exercise 3` why the representation of `good` has high cosine similarity in sentence 0 and sentence 1, while similarity of both of token `good` in both the sentences 0,1  is low when compared to `good` of sentence 2.\n",
        "\n",
        "2. How the representations obtained here differ from the token representation of GLOVE.\n",
        "\n"
      ],
      "metadata": {
        "id": "IsW8sb86-qno"
      }
    }
  ]
}