{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"0be6c75477e641298e2c49e331877267":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_a8c5fb18a7b947a5bd1f0486e5ab4a95","IPY_MODEL_44c0c9f1886b435ba785364bca4daab7","IPY_MODEL_876d4a6b52374597a7431ff552a46a93"],"layout":"IPY_MODEL_165b161db58249b49d9939e0e3bc1183"}},"a8c5fb18a7b947a5bd1f0486e5ab4a95":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_cb6c89d0bd1c4a7fb0923d24249feccc","placeholder":"​","style":"IPY_MODEL_bf3493e51fa24eaeacded255f177ed96","value":"Pandas Apply: 100%"}},"44c0c9f1886b435ba785364bca4daab7":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_9f85447882db4921a03860a35c91dfd1","max":2320,"min":0,"orientation":"horizontal","style":"IPY_MODEL_6d3e9f66e98b4775b654ea3288a48034","value":2320}},"876d4a6b52374597a7431ff552a46a93":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_91ae200711ed4421bd02bf2febdecdb3","placeholder":"​","style":"IPY_MODEL_35c1916005064936bd45e35813fcabbd","value":" 2320/2320 [00:02&lt;00:00, 1253.83it/s]"}},"165b161db58249b49d9939e0e3bc1183":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cb6c89d0bd1c4a7fb0923d24249feccc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bf3493e51fa24eaeacded255f177ed96":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9f85447882db4921a03860a35c91dfd1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6d3e9f66e98b4775b654ea3288a48034":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"91ae200711ed4421bd02bf2febdecdb3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"35c1916005064936bd45e35813fcabbd":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6a142002dd0d463282250103f5263d25":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_1fa9b2118f5c4ac380564068cf53d2a6","IPY_MODEL_f472055da96c45b1b6abca004ecb8a4f","IPY_MODEL_1d9728f4b92a4a5ebe5fc9c12f4c4e6b"],"layout":"IPY_MODEL_da641152b1fb4404a56d33d55558cec4"}},"1fa9b2118f5c4ac380564068cf53d2a6":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ad5795cfd6ce4251ba978eb7bc763113","placeholder":"​","style":"IPY_MODEL_30e493a309ba42d59cba849cfa6c35a6","value":"Pandas Apply: 100%"}},"f472055da96c45b1b6abca004ecb8a4f":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_e241d82db50c4971a5288df16e7f5b77","max":2320,"min":0,"orientation":"horizontal","style":"IPY_MODEL_54042889f44246d1984b0ddc8060ce5c","value":2320}},"1d9728f4b92a4a5ebe5fc9c12f4c4e6b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_14d9f2ef848f4548ba6ce9fbe12ed205","placeholder":"​","style":"IPY_MODEL_0d3bd72486094d5787bcd03f46acf5b8","value":" 2320/2320 [00:02&lt;00:00, 1262.63it/s]"}},"da641152b1fb4404a56d33d55558cec4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ad5795cfd6ce4251ba978eb7bc763113":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"30e493a309ba42d59cba849cfa6c35a6":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e241d82db50c4971a5288df16e7f5b77":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"54042889f44246d1984b0ddc8060ce5c":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"14d9f2ef848f4548ba6ce9fbe12ed205":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0d3bd72486094d5787bcd03f46acf5b8":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"markdown","metadata":{"id":"79MQN4eMlBCq"},"source":["# Learning Objectives\n","In this lab we are going to:\n","- Explore using semantic features to improve intent classification\n","- Explore word sense disambiguation (WSD) with Wordnet/NLTK\n","- WSD using the lesk algorithm\n"]},{"cell_type":"markdown","metadata":{"id":"3Zs1UOAvkKl6"},"source":["# Setup \n","Please run all the cell in this section before starting exercises. This section install the necessary packages and downloads the data needed for all exercises below."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jP-UE7vYPkJG","outputId":"d9c7a740-f82b-4a7d-ae77-62a8122d463e","executionInfo":{"status":"ok","timestamp":1667471298972,"user_tz":0,"elapsed":9911,"user":{"displayName":"Shardul Suryawanshi","userId":"16657988701249956661"}}},"source":["!pip install datasets >> dev.null\n","!pip install swifter >> dev.null\n","\n","import nltk\n","from nltk.corpus import wordnet as wn\n","from nltk.wsd import lesk\n","from nltk import word_tokenize\n","\n","nltk.download('wordnet')\n","nltk.download('punkt')\n","nltk.download('brown')\n","nltk.download('averaged_perceptron_tagger')\n","nltk.download('omw-1.4')\n","\n","print(\"finished installing\")"],"execution_count":8,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package brown to /root/nltk_data...\n","[nltk_data]   Package brown is already up-to-date!\n","[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n","[nltk_data]       date!\n","[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"]},{"output_type":"stream","name":"stdout","text":["finished installing\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8zMTdojpkgjc","outputId":"3d61f986-f739-4ca5-8abc-34ba28f9b99b","executionInfo":{"status":"ok","timestamp":1667471184437,"user_tz":0,"elapsed":1150,"user":{"displayName":"Shardul Suryawanshi","userId":"16657988701249956661"}}},"source":["# Grab Clinc data and convert them into data frames\n","import urllib.request, json \n","with urllib.request.urlopen(\"https://raw.githubusercontent.com/clinc/oos-eval/master/data/data_small.json\") as url:\n","    data = json.loads(url.read().decode())\n","\n","import pandas as pd \n","\n","train  = pd.DataFrame(data[\"train\"], columns=[\"text\", \"intent\"])\n","train[\"split\"] = \"train\"\n","\n","test   = pd.DataFrame(data[\"test\"], columns=[\"text\", \"intent\"])\n","test[\"split\"] = \"test\"\n","\n","print(f\"dataset split sizes: train - {len(train)} | test - {len(test)}\")\n","\n","# Combine datasets into single dataframe for easier use\n","dataset = pd.concat([train, test])\n","\n","# Filter intents \n","labels = [\"application_status\", \"alarm\", \"apr\", \"are_you_a_bot\", \n","          \"balance\", \"calendar_update\", \"calories\", \"carry_on\", \"change_accent\",\n","          \"change_ai_name\", \"change_language\", \"change_volume\", \"exchange_rate\",\n","          \"expiration_date\", \"find_phone\", \"freeze_account\", \"greeting\",\n","          \"insurance_change\", \"jump_start\",\n","          \"interest_rate\",\n","          \"smart_home\",\n","          \"schedule_meeting\",\n","          \"user_name\",\n","          \"w2\",\n","          \"weather\",\n","          \"what_is_your_name\",\n","          \"what_song\",\n","          \"who_do_you_work_for\",\n","          \"yes\"\n","          ]\n","dataset = dataset.query(\"intent in @labels\")\n","\n","print(\"finished\")"],"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["dataset split sizes: train - 7500 | test - 4500\n","finished\n"]}]},{"cell_type":"markdown","metadata":{"id":"rgWrbXAylXH4"},"source":["# Overview\n","\n","In this lab we'll be learning more about semantic features through the applied task of intent classification. Intent classification is a common feature of most modern conversational AI and chatbot systems. Intent classification aims to understand what a user is telling the bot and map the user's utterance to a prespecified set of intent categories. Let's imagine we're buidling a chatbot for a bank to handle common banking actions like: opening a savings account (open-savings) or reset pin number (reset-pin). We can can create a set of intents like open-savings and reset-pin to represent those actions to the bot. Then when a user says \"I want to open a savings account\" or \"How can I sign up for a savings account\", our classifier model will predict \"open-savings\" intent and respond to the user accordingly. \n","\n","We can use machine learning to build a classifier that can automatically map user utterances to intents. For this lab we'll explore creating a simple model for intent classification and using semantic features (e.g. synsets from wordnet and word sense disambiguation) to improve the perfomance our baseline model. \n","\n","## Data\n","We'll be using the Clinc-150 dataset which consists of 150 common chatbot intents and training utterances. More information about this dataset can be found in their paper [An Evaluation Dataset for Intent Classification and Out-of-Scope Prediction](https://aclanthology.org/D19-1131/). \n","\n","For simplicity we gone ahead downloaded the data for you and stored it in a pandas dataframe. The variable `dataset` is pandas dataframe consisting of three columns:\n","- text: the user utterance \n","- intent: intent label \n","- split: identifies if the row is from the train or test split\n","\n","\n","We'll use a smaller subset of 150 intents so that we can experiment more quickly. As the goal of the lab is on semantic analysis, we've provided the baseline model for you. However you are encouraged to explore the full dataset and other models once you've finished the exercises below. \n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":424},"id":"HaAXI35kwZoH","outputId":"e014596f-c993-4958-8e47-9a8c94536c5b","executionInfo":{"status":"ok","timestamp":1667471184444,"user_tz":0,"elapsed":135,"user":{"displayName":"Shardul Suryawanshi","userId":"16657988701249956661"}}},"source":["# Overview of dataset variable\n","dataset"],"execution_count":3,"outputs":[{"output_type":"execute_result","data":{"text/plain":["                                                   text      intent  split\n","50    what am i allowed to carry on for american air...    carry_on  train\n","51               what are the carry on rules for united    carry_on  train\n","52                       what can't i carry-on to delta    carry_on  train\n","53                     tell me united's carry on policy    carry_on  train\n","54                           what is the carry on limit    carry_on  train\n","...                                                 ...         ...    ...\n","4285   what do i need to do now that my battery is dead  jump_start   test\n","4286                        i have to jump start my car  jump_start   test\n","4287        what shall i do now that my battery is dead  jump_start   test\n","4288                          i gotta jump start my car  jump_start   test\n","4289       what is the process for jump starting my car  jump_start   test\n","\n","[2320 rows x 3 columns]"],"text/html":["\n","  <div id=\"df-e73992fb-8dfa-48e5-9c0f-011b3b683d5c\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>text</th>\n","      <th>intent</th>\n","      <th>split</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>50</th>\n","      <td>what am i allowed to carry on for american air...</td>\n","      <td>carry_on</td>\n","      <td>train</td>\n","    </tr>\n","    <tr>\n","      <th>51</th>\n","      <td>what are the carry on rules for united</td>\n","      <td>carry_on</td>\n","      <td>train</td>\n","    </tr>\n","    <tr>\n","      <th>52</th>\n","      <td>what can't i carry-on to delta</td>\n","      <td>carry_on</td>\n","      <td>train</td>\n","    </tr>\n","    <tr>\n","      <th>53</th>\n","      <td>tell me united's carry on policy</td>\n","      <td>carry_on</td>\n","      <td>train</td>\n","    </tr>\n","    <tr>\n","      <th>54</th>\n","      <td>what is the carry on limit</td>\n","      <td>carry_on</td>\n","      <td>train</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>4285</th>\n","      <td>what do i need to do now that my battery is dead</td>\n","      <td>jump_start</td>\n","      <td>test</td>\n","    </tr>\n","    <tr>\n","      <th>4286</th>\n","      <td>i have to jump start my car</td>\n","      <td>jump_start</td>\n","      <td>test</td>\n","    </tr>\n","    <tr>\n","      <th>4287</th>\n","      <td>what shall i do now that my battery is dead</td>\n","      <td>jump_start</td>\n","      <td>test</td>\n","    </tr>\n","    <tr>\n","      <th>4288</th>\n","      <td>i gotta jump start my car</td>\n","      <td>jump_start</td>\n","      <td>test</td>\n","    </tr>\n","    <tr>\n","      <th>4289</th>\n","      <td>what is the process for jump starting my car</td>\n","      <td>jump_start</td>\n","      <td>test</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>2320 rows × 3 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e73992fb-8dfa-48e5-9c0f-011b3b683d5c')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-e73992fb-8dfa-48e5-9c0f-011b3b683d5c button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-e73992fb-8dfa-48e5-9c0f-011b3b683d5c');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":3}]},{"cell_type":"markdown","metadata":{"id":"NglQul4ym7zT"},"source":["## Baseline Model Preprocessing and setup\n","\n","Recall from previous lectures that building a machine learning NLP model consists of several steps. \n","\n","- First we encode our categorical labels into numerical values that our classifier will be predicting. To accomplish this we use sklearns LabelEncoder (https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html). We generate two variables `y_train` and `y_test` which contain our encoded labels in to list.\n","\n","- Next we create input features for our model. As the input to our model is user utterances (text), we'll represent each sentence as a vector of `tf-idf` scores across a learned vocabulary. We use the `TfidfVectorizer` (https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) to fit and transform our user utterances into tfidf features.\n","\n"]},{"cell_type":"code","metadata":{"id":"SoWDefT0lnLK","colab":{"base_uri":"https://localhost:8080/"},"outputId":"2ad8ee37-f91d-4434-8ff0-c677c2a6e0f2","executionInfo":{"status":"ok","timestamp":1667471184446,"user_tz":0,"elapsed":75,"user":{"displayName":"Shardul Suryawanshi","userId":"16657988701249956661"}}},"source":["from sklearn.preprocessing import LabelEncoder\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","\n","# Encode intent label and transform into enumerated values\n","le = LabelEncoder()\n","le.fit(dataset[\"intent\"])\n","\n","dataset[\"encoded_label\"] = le.transform(dataset[\"intent\"])\n","\n","# break out the encoded labels by train / test split\n","y_train = dataset.query(\"split=='train'\")[\"encoded_label\"]  \n","y_test  = dataset.query(\"split=='test'\")[\"encoded_label\"]\n","\n","# Generate a vocabaulary from text and generate tfidf features \n","tfidf = TfidfVectorizer()\n","tfidf.fit(dataset[\"text\"])\n","\n","# create train and text input features\n","X_train = tfidf.transform(dataset.query(\"split=='train'\")[\"text\"])\n","X_test =  tfidf.transform(dataset.query(\"split=='test'\")[\"text\"])\n","\n","print(\"finished\")"],"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["finished\n"]}]},{"cell_type":"markdown","metadata":{"id":"EyU2kCJWy0lA"},"source":["## Baseline Model Implementation\n","\n","Finally we go ahead and train our model. We'll use Gaussian Naive Bayes(https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html) with the default hyper parameters. Feel free to read the documention to experiment with the hyperparemeters and see how it effects the model.\n","\n","\n","Finally we'll evaluate the model on our test set by first generating predictions used the trained model and then evaluating them against our gold labels `y_test`. We use sklearn's `accuracy_score` to get the accuracy of our model"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lihpfssXoilR","outputId":"89a99437-432e-44c4-8a0b-3d0d2b9a240d","executionInfo":{"status":"ok","timestamp":1667471184725,"user_tz":0,"elapsed":334,"user":{"displayName":"Shardul Suryawanshi","userId":"16657988701249956661"}}},"source":["from sklearn.naive_bayes import GaussianNB\n","from sklearn.metrics import accuracy_score\n","\n","# Load model and fit to training data\n","clf = GaussianNB()\n","clf.fit(X_train.toarray(), y_train)\n","\n","# Generate predictions\n","preds = clf.predict(X_test.toarray())\n","\n","# Calculate Accuracy metrics\n","acc = accuracy_score(y_test, preds)\n","print(f\"Accuracy - {acc}\")"],"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy - 0.7988505747126436\n"]}]},{"cell_type":"markdown","metadata":{"id":"3ykMx_7kswTA"},"source":["# Semantic features with Wordnet\n","\n","Let's get to main exerises for this week. Recall that semantics involves understanding the meaning of words and concepts. Semantic analysis of text explores specific meaning of words in context (word sense), words that related together through common definitions (synoynms) or perhaps diameterically different from each other (antonyms) and in many other ways. In addition to the slides, as useful introduction can be found here: [Overview and Semantic Issues of Text Mining](https://sigmodrecord.org/publications/sigmodRecord/0709/p23.cesar-andritsos.pdf)\n","\n","## Wordnet Synsets\n","We'll be using WordNet as the source of our semantic features and tools. \n","\n","\n","\"WordNet is an on-line lexical reference system whose design is inspired by current psycholinguistic theories of human lexical memory. English nouns, verbs, and adjectives are organized into synonym sets, each representing one underlying lexical concept. Different relations link the synonym sets.\" [1]\n","\n","The NLTK library has a useful set of APIs to access Wordnet and perform common semantic analysis tasks. More information can be found in the documentation here: https://www.nltk.org/howto/wordnet.html.\n","\n","In this exercise we'll be generating synoynms from wordnet synsets. The goal is augment the input to the model with additional synonyms for the key nouns to give the model additional semantic clues as to the meaning of the input. \n","\n","Synsets are an NLTK object which are a group of synonymous words that are related to a specific concept. \n","\n","The synset object has several useful properties and methods. Each synset contains a dictionary definition, related synonyms, and many other properties that can be found in the documentation.  \n","\n","Let's a take a closer look in the code below:\n","\n","[1] Introduction to WordNet: An On-line Lexical Database (https://wordnetcode.princeton.edu/5papers.pdf)\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jZEg4OZHwXKM","outputId":"94aafc2f-75f5-46c3-9a15-ed536fcbbc07","executionInfo":{"status":"ok","timestamp":1667471325854,"user_tz":0,"elapsed":2187,"user":{"displayName":"Shardul Suryawanshi","userId":"16657988701249956661"}}},"source":["from nltk.corpus import wordnet as wn\n","\n","# Let's take a looking at the synset for the word balance in Wordnet\n","\n","# Synsets is a collection of synset obects. Here the first 5 synsets related to \n","# the word balance\n","for synset in wn.synsets(\"balance\")[:5]:\n","  print(synset)\n","  print(\"Defintion: \",synset.definition())\n","  print(\"Synonym lemmas: \", synset.lemma_names() )\n","  print(\"-----------------------------------\")"],"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Synset('balance.n.01')\n","Defintion:  a state of equilibrium\n","Synonym lemmas:  ['balance']\n","-----------------------------------\n","Synset('balance.n.02')\n","Defintion:  equality between the totals of the credit and debit sides of an account\n","Synonym lemmas:  ['balance']\n","-----------------------------------\n","Synset('proportion.n.05')\n","Defintion:  harmonious arrangement or relation of parts or elements within a whole (as in a design); - John Ruskin\n","Synonym lemmas:  ['proportion', 'proportionality', 'balance']\n","-----------------------------------\n","Synset('balance.n.04')\n","Defintion:  equality of distribution\n","Synonym lemmas:  ['balance', 'equilibrium', 'equipoise', 'counterbalance']\n","-----------------------------------\n","Synset('remainder.n.01')\n","Defintion:  something left after other parts have been taken away\n","Synonym lemmas:  ['remainder', 'balance', 'residual', 'residue', 'residuum', 'rest']\n","-----------------------------------\n"]}]},{"cell_type":"markdown","metadata":{"id":"7SCq13iw7lL4"},"source":["# Exercise 1\n","\n","The overall goal of this exercise is to agument each input sentence with a set of related synonyms for the nouns found in the sentence. At a high level we can accomplish this with the following steps:\n","\n","1. Extract all nouns in the sentence\n","2. For each noun look up synonyms (if they exist) from the Wordnet synsets\n","3. Add the extracted synonyms to the end of the input"]},{"cell_type":"markdown","metadata":{"id":"uOLorYsu6Qom"},"source":["## Exercise 1a. Extract all nouns in a sentence\n","For this exercise you will write a function that extracts all the nouns in a sentece. \n","\n","For example:\n","\n","- Sentence: I want to check the balance of my bank account\n","- Nouns: balance, bank, account \n","\n","To identify nouns we can use a NLTK POS tagger to tag each word in the sentence with POS tag. We can then extract all word with the relevant POS tags that define nouns. Below we show how you can get POS tags from NLTK for a given sentence"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eodOlApnRBMm","outputId":"1a2b8d16-8a1b-4ac4-a51f-4f73e0daa378","executionInfo":{"status":"ok","timestamp":1667471332803,"user_tz":0,"elapsed":468,"user":{"displayName":"Shardul Suryawanshi","userId":"16657988701249956661"}}},"source":["import nltk\n","from nltk import word_tokenize\n","\n","ex1 = \"I want to check the balance of my bank accounts\"\n","\n","# 1. Tokenize text\n","toks = word_tokenize(ex1)\n","\n","# 2. Generate tagged tokens\n","tagged = nltk.pos_tag(toks)\n","\n","# 3. Loop over tagged tokens and print out word and tag\n","for word, pos_tag in tagged:\n","  print(word, pos_tag)"],"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["I PRP\n","want VBP\n","to TO\n","check VB\n","the DT\n","balance NN\n","of IN\n","my PRP$\n","bank NN\n","accounts NNS\n"]}]},{"cell_type":"markdown","metadata":{"id":"xUfTFXuDG8hh"},"source":["Go ahead and write the code to extract the nouns based on POS tags. For a full list of POS tags see: https://pythonprogramming.net/natural-language-toolkit-nltk-part-speech-tagging/ \n","\n","\n","Hint: the POS tag for singular nouns is NN. Can you find the remaining tags for plural nouns, proper nouns singular and proper noun plural. "]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EATlO4-NVk-n","outputId":"90808d76-2833-4cf1-a15b-d72dba5e210c","executionInfo":{"status":"ok","timestamp":1667471337009,"user_tz":0,"elapsed":499,"user":{"displayName":"Shardul Suryawanshi","userId":"16657988701249956661"}}},"source":["def extract_nouns(text):\n","  \"\"\"\n","  This method takes in a sentene and return back of extracted nouns. If no nouns\n","  are found, an empty list is returned. \n","  \"\"\"\n","  # 1. Tokenize text\n","  toks = nltk.word_tokenize(text)\n"," \n","  # 2. Loop over tagged tokens and extract nouns. Add extracted nouns to the\n","  # extracted nouns list\n","  noun_tags = ['NN', 'NNP', 'NNS', 'NNPS']\n","  extracted_nouns = []\n","  \n","  # YOUR CODE BELOW\n","  for word, pos_tag in nltk.pos_tag(toks):\n","    if pos_tag in noun_tags:\n","      extracted_nouns.append(word)\n","  return extracted_nouns\n","\n","extract_nouns(ex1)"],"execution_count":11,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['balance', 'bank', 'accounts']"]},"metadata":{},"execution_count":11}]},{"cell_type":"markdown","metadata":{"id":"TxdZJFgpS6yO"},"source":["## Exercise 1b. Extract synonyms from Wordnet \n","Next you will create function to extract all the synonymns for a provided work from Wordnet. In the function below write your code to get synonyms from from Wordnet.\n","\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"S7tdPc6hW51a","outputId":"beeb7033-57de-479d-9676-bceead2d6a0c","executionInfo":{"status":"ok","timestamp":1667471340366,"user_tz":0,"elapsed":383,"user":{"displayName":"Shardul Suryawanshi","userId":"16657988701249956661"}}},"source":["def extract_synonyms(noun):\n","  \"\"\"\n","  Given a noun, return all the synonyms (if any) found in the synsets for\n","  that noun. Return an empty list if none are found. \n","  \"\"\"\n","  synonyms = []\n","\n","  # Loop over the synsets for a given word and add each synonym \n","  # to the synonyms list. \n","  for synset in wn.synsets(noun):\n","    synonyms.append(synset.lemma_names()[0].replace(\"_\", \" \"))\n","  return(synonyms)\n","\n","extract_synonyms(\"balance\")  "],"execution_count":12,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['balance',\n"," 'balance',\n"," 'proportion',\n"," 'balance',\n"," 'remainder',\n"," 'balance',\n"," 'Libra',\n"," 'Libra',\n"," 'symmetry',\n"," 'counterweight',\n"," 'balance wheel',\n"," 'balance',\n"," 'balance',\n"," 'balance',\n"," 'poise',\n"," 'balance']"]},"metadata":{},"execution_count":12}]},{"cell_type":"markdown","metadata":{"id":"amfFZ4Spbs2W"},"source":["## Exercise 1c. Add extracted synonyms to end of input\n","\n","Finally, you will create a function that combines the functions you wote in 1a and 1b. In this function, you'll take all the synonyms found and add them to the end of the input. \n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"e2xG8j7EkDVF","outputId":"a6f21176-3cd3-48d6-9926-1ee4502647f7","executionInfo":{"status":"ok","timestamp":1667471345398,"user_tz":0,"elapsed":541,"user":{"displayName":"Shardul Suryawanshi","userId":"16657988701249956661"}}},"source":["def add_synonyms_to_text(text: str) -> str:\n","  all_synonyms = []\n"," \n","  # 1. Extract Nouns\n","  extracted_nouns = extract_nouns(text)\n","\n","  # Edge case, no nouns are found\n","  if extracted_nouns == []:\n","    return text\n","\n","  for noun in extracted_nouns:\n","    # 1. Get synonyms\n","    s = extract_synonyms(noun)\n","    \n","    # 2. Add it to our list of synoyms\n","    all_synonyms.extend(s)\n","    unique_synonyms = set(all_synonyms)\n","    \n","  # 3. Return input with extracted synonyms appended at the end\n","  new_text = text + \" \" + \" \".join(unique_synonyms)\n","  if new_text is None:\n","    return text\n","  else:\n","    return new_text\n","\n","print(f\"original text: {ex1}\")\n","print(f\"new text: {add_synonyms_to_text(ex1)}\")"],"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["original text: I want to check the balance of my bank accounts\n","new text: I want to check the balance of my bank accounts score bank account poise symmetry remainder explanation counterweight balance wheel Libra bill depository financial institution history report savings bank balance proportion deposit trust\n"]}]},{"cell_type":"markdown","metadata":{"id":"1EO6hZaYcuiA"},"source":["## Evaluation of Synonym features\n","\n","Now we're ready to see what the effect of these semantic feature are on our model. We will want to apply the `add_synonyms_to_text` method to all the \n","text in our dataset. Doing this linearly in a for loop is very inefficient. So provide code that parallelizes the operation below. We also provide the code to run to retrain the model. After running the cells below go to Exercise 1d to wrap up Exercise 1. "]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":104,"referenced_widgets":["0be6c75477e641298e2c49e331877267","a8c5fb18a7b947a5bd1f0486e5ab4a95","44c0c9f1886b435ba785364bca4daab7","876d4a6b52374597a7431ff552a46a93","165b161db58249b49d9939e0e3bc1183","cb6c89d0bd1c4a7fb0923d24249feccc","bf3493e51fa24eaeacded255f177ed96","9f85447882db4921a03860a35c91dfd1","6d3e9f66e98b4775b654ea3288a48034","91ae200711ed4421bd02bf2febdecdb3","35c1916005064936bd45e35813fcabbd"]},"id":"2lgPnThjalAG","outputId":"2e436e4f-3f5b-41ed-c9c2-1371bd3a2992","executionInfo":{"status":"ok","timestamp":1667471356615,"user_tz":0,"elapsed":3514,"user":{"displayName":"Shardul Suryawanshi","userId":"16657988701249956661"}}},"source":["import swifter\n","\n","# Apply the text with synonyms function to text column in datasets\n","dataset[\"text_with_synonyms\"] = dataset[\"text\"].swifter.apply(lambda x: add_synonyms_to_text(x)) "],"execution_count":14,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/swifter/swifter.py:88: UserWarning: This pandas object has duplicate indices, and swifter may not be able to improve performance. Consider resetting the indices with `df.reset_index(drop=True)`.\n","  \"This pandas object has duplicate indices, \"\n"]},{"output_type":"display_data","data":{"text/plain":["Pandas Apply:   0%|          | 0/2320 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0be6c75477e641298e2c49e331877267"}},"metadata":{}}]},{"cell_type":"code","metadata":{"id":"bzMjPHZWxul0","executionInfo":{"status":"ok","timestamp":1667471356618,"user_tz":0,"elapsed":31,"user":{"displayName":"Shardul Suryawanshi","userId":"16657988701249956661"}}},"source":["# Generate inputs \n","tfidf_w_synonyms = TfidfVectorizer()\n","tfidf_w_synonyms.fit(dataset[\"text_with_synonyms\"])\n","\n","X_train = tfidf_w_synonyms.transform(dataset.query(\"split=='train'\")[\"text_with_synonyms\"])\n","X_test =  tfidf_w_synonyms.transform(dataset.query(\"split=='test'\")[\"text_with_synonyms\"])"],"execution_count":15,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3J6_cCCOJlrN","outputId":"19fc6b4e-4811-4442-ab4e-ceeb1bd51538","executionInfo":{"status":"ok","timestamp":1667471357909,"user_tz":0,"elapsed":808,"user":{"displayName":"Shardul Suryawanshi","userId":"16657988701249956661"}}},"source":["from sklearn.naive_bayes import GaussianNB\n","from sklearn.metrics import accuracy_score\n","\n","clf = GaussianNB()\n","clf.fit(X_train.toarray(), y_train)\n","\n","preds = clf.predict(X_test.toarray())\n","print(accuracy_score(y_test, preds))"],"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["0.8057471264367816\n"]}]},{"cell_type":"markdown","metadata":{"id":"-y14bzHQfJbq"},"source":["## Exercise 1d: Evaluation\n","Did adding synonyms to the input improve performance? What is a potential downside of this approach?\n","\n","Write your answer here:\n","We see a minor improvement (1pp gain) in accuracy. A downside to this approach is that we may adding irrelevant synoyms to the input as nouns could have multiple meanings. "]},{"cell_type":"markdown","metadata":{"id":"SP2E-BeENNW3"},"source":["# Improving Synonym Selection with WSD and Lesk\n","\n","One limitation of the approach above was that we were naively adding all the synonyms we found in a synset for a given word. However, words can have different meanings in different contexts. Recall the concept of word sense and the problem of word sense disambiguation. Word sense disambiguation aims to identify the specific meaning of a word in context. \n","\n","The lesk algorithm aims to disambugiuate a word based on the overlap between the context around the word and the definitions of the word. The definition that has the highest similarity to the conctext is returned. \n","\n","NLTK has an implementation of the lesk algorithm which we explore below."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"k1Fil0hYQtYx","outputId":"e59e42d0-ea28-4725-b14a-03f70362bb3d","executionInfo":{"status":"ok","timestamp":1667471362694,"user_tz":0,"elapsed":381,"user":{"displayName":"Shardul Suryawanshi","userId":"16657988701249956661"}}},"source":["from nltk.wsd import lesk\n","\n","sent = 'He cashed a check at the bank.'\n","sent_toks = nltk.word_tokenize(sent)\n","ambiguous = 'bank'\n","\n","syn = lesk(sent_toks, ambiguous, pos='n')\n","print(sent)\n","print(syn)\n","print(\"Definition: \", syn.definition())\n","print(\"Synonyms: \", syn.lemma_names())\n","\n","print(\"\\n------------------------------------------\\n\")\n","sent2 = 'He saw the road bank left.'\n","sent2_toks = nltk.word_tokenize(sent2)\n","ambiguous = 'bank'\n","syn2 = lesk(sent2_toks, \"bank\")\n","\n","print(sent2)\n","print(syn2)\n","print(\"Definition: \", syn2.definition())\n","print(\"Synonyms: \", syn2.lemma_names())"],"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["He cashed a check at the bank.\n","Synset('savings_bank.n.02')\n","Definition:  a container (usually with a slot in the top) for keeping money at home\n","Synonyms:  ['savings_bank', 'coin_bank', 'money_box', 'bank']\n","\n","------------------------------------------\n","\n","He saw the road bank left.\n","Synset('bank.n.07')\n","Definition:  a slope in the turn of a road or track; the outside is higher than the inside in order to reduce the effects of centrifugal force\n","Synonyms:  ['bank', 'cant', 'camber']\n"]}]},{"cell_type":"markdown","metadata":{"id":"WP1GS1GFpof2"},"source":["## Exercise 2\n","\n","Let's create a function `lesk_synonyms` that provides synonyms based on the disambiguated noun. "]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eyfcM7vYW6Yt","outputId":"0a377c48-b56c-41d0-86c8-da50a42ad7fe","executionInfo":{"status":"ok","timestamp":1667471384666,"user_tz":0,"elapsed":249,"user":{"displayName":"Shardul Suryawanshi","userId":"16657988701249956661"}}},"source":["\n","def lesk_synonyms(text, ambiguous):\n","  \"\"\"\n","  \"\"\"\n","  # Get disambiguated synset\n","  syn = lesk(text, ambiguous, pos='n')\n","\n","  # Edge case, if synset is empty return empty list\n","  if syn is None:\n","    return []\n","\n","  # Strip out '_' found in lemmas that contain compound words.\n","  cleaned_synonyms = [lemma.replace(\"_\", \" \") for lemma in syn.lemma_names()] \n","  return cleaned_synonyms\n","\n","lesk_synonyms(\"I went to the bank today.\", \"bank\")"],"execution_count":19,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['savings bank', 'coin bank', 'money box', 'bank']"]},"metadata":{},"execution_count":19}]},{"cell_type":"markdown","metadata":{"id":"yknQEHOhqqci"},"source":["We updated the `add_synonyms_to_text` to use the lesk synonyms and updated the model below. After running the cells below, go Exercise 2a. "]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xRW8P1hWZ8ZY","outputId":"c2d68bfd-c90f-488e-f6cf-08153a862b7d","executionInfo":{"status":"ok","timestamp":1667471389197,"user_tz":0,"elapsed":350,"user":{"displayName":"Shardul Suryawanshi","userId":"16657988701249956661"}}},"source":["def add_synonyms_to_text(text): \n","  \n","  # 1. Extract Nouns\n","  extracted_nouns = extract_nouns(text)\n","\n","  # Edge case, no nouns are found\n","  if extracted_nouns == []:\n","    return text\n","\n","  all_synonyms = []\n","  for noun in extracted_nouns:\n","    # 1. Get synonyms\n","    s = lesk_synonyms(text, noun)\n","    \n","    # 2. Add it to our list of synoyms\n","    all_synonyms.extend(s)\n","    unique_synonyms = set(all_synonyms)\n","  \n","  if len(unique_synonyms) == 0:\n","    return text\n","  else:\n","   return text + \" \" + \" \".join(unique_synonyms)\n","\n","print(\"He went to the bank\")\n","print(add_synonyms_to_text(\"He went to the bank\"))"],"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["He went to the bank\n","He went to the bank money box savings bank bank coin bank\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":104,"referenced_widgets":["6a142002dd0d463282250103f5263d25","1fa9b2118f5c4ac380564068cf53d2a6","f472055da96c45b1b6abca004ecb8a4f","1d9728f4b92a4a5ebe5fc9c12f4c4e6b","da641152b1fb4404a56d33d55558cec4","ad5795cfd6ce4251ba978eb7bc763113","30e493a309ba42d59cba849cfa6c35a6","e241d82db50c4971a5288df16e7f5b77","54042889f44246d1984b0ddc8060ce5c","14d9f2ef848f4548ba6ce9fbe12ed205","0d3bd72486094d5787bcd03f46acf5b8"]},"id":"LvmW9Handm-V","outputId":"4d41cd6e-cf36-4f14-abb6-f8588c08db12","executionInfo":{"status":"ok","timestamp":1667471397563,"user_tz":0,"elapsed":4375,"user":{"displayName":"Shardul Suryawanshi","userId":"16657988701249956661"}}},"source":["dataset[\"text_with_lesk_synonyms\"] = dataset[\"text\"].swifter.apply(lambda x: add_synonyms_to_text(x)) "],"execution_count":21,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/swifter/swifter.py:88: UserWarning: This pandas object has duplicate indices, and swifter may not be able to improve performance. Consider resetting the indices with `df.reset_index(drop=True)`.\n","  \"This pandas object has duplicate indices, \"\n"]},{"output_type":"display_data","data":{"text/plain":["Pandas Apply:   0%|          | 0/2320 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6a142002dd0d463282250103f5263d25"}},"metadata":{}}]},{"cell_type":"code","metadata":{"id":"9PCSBRrpe-0B","colab":{"base_uri":"https://localhost:8080/"},"outputId":"51b1ba43-6030-48e5-a921-179f609a9f22","executionInfo":{"status":"ok","timestamp":1667471398272,"user_tz":0,"elapsed":77,"user":{"displayName":"Shardul Suryawanshi","userId":"16657988701249956661"}}},"source":["# Generate inputs \n","tfidf_w_synonyms = TfidfVectorizer()\n","tfidf_w_synonyms.fit(dataset[\"text_with_lesk_synonyms\"])\n","\n","X_train = tfidf_w_synonyms.transform(dataset.query(\"split=='train'\")[\"text_with_lesk_synonyms\"])\n","X_test =  tfidf_w_synonyms.transform(dataset.query(\"split=='test'\")[\"text_with_lesk_synonyms\"])\n","\n","print(\"finished\")"],"execution_count":22,"outputs":[{"output_type":"stream","name":"stdout","text":["finished\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VKWWTG-Eg2Wn","outputId":"1a3ef341-3081-4e08-d89d-977e55b8a17e","executionInfo":{"status":"ok","timestamp":1667471402841,"user_tz":0,"elapsed":430,"user":{"displayName":"Shardul Suryawanshi","userId":"16657988701249956661"}}},"source":["from sklearn.naive_bayes import GaussianNB\n","from sklearn.metrics import accuracy_score\n","clf = GaussianNB()\n","clf.fit(X_train.toarray(), y_train)\n","\n","preds = clf.predict(X_test.toarray())\n","print(accuracy_score(preds, y_test))"],"execution_count":23,"outputs":[{"output_type":"stream","name":"stdout","text":["0.8402298850574713\n"]}]},{"cell_type":"markdown","metadata":{"id":"q1LJsdpXrs0K"},"source":["## Exercise 2a. Evaluation\n","What effect did replacing synonyms with lesk synonyms have on the model's accuracy?"]},{"cell_type":"code","metadata":{"id":"Y9PJcGRssADu","executionInfo":{"status":"ok","timestamp":1667471407216,"user_tz":0,"elapsed":255,"user":{"displayName":"Shardul Suryawanshi","userId":"16657988701249956661"}}},"source":[],"execution_count":23,"outputs":[]}]}