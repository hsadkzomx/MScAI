{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2G6saNLUFqHb"
   },
   "source": [
    "# Assignment 2 - CT5120\n",
    "\n",
    "### Instructions:\n",
    "- Complete all the tasks below and upload your submission as a Python notebook on Blackboard with the filename “`StudentID_Lastname.ipynb`” before **23:59** on **November 25, 2022**.\n",
    "- This is an individual assignment, you **must not** work with other students to complete this assessment.\n",
    "- The assignment is worth $50$ marks and constitutes 19% of the final grade. The breakdown of the marking scheme for each task is as follows:\n",
    "\n",
    "| Task | Marks for write-up | Marks for code | Total Marks |\n",
    "| :--- | :----------------- | :------------- | :---------- |\n",
    "| 1    |                  5 |              5 |          10 |\n",
    "| 2    |                  - |             10 |          10 |\n",
    "| 3    |                  5 |              5 |          10 |\n",
    "| 4    |                  5 |              5 |          10 |\n",
    "| 5    |                  5 |              5 |          10 |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZCWSEtNeGMsN"
   },
   "source": [
    "---\n",
    "\n",
    "This assignment involves tasks for feature engineering, training and evaluating a classifier for suggestion detection. You will work with the data from SemEval-2019 Task 9 subtask A to classify whether a piece of text contains a suggestion or not. \n",
    "\n",
    "\n",
    "Download train.csv, test_seen.csv and test_unseen.csv from the [Github](https://github.com/sharduls007/Assignment_2_CT5120) or uncomment the code cell below to get the data as a comma-separated values (CSV) file. The CSV file contains a header row followed by 5,440 rows in train.csv and 1,360 rows in test_seen.csv spread across 3 columns of data. Each row of data contains a unique id, a piece of text and a label assigned by an annotator. A label of $1$ indicates that the given text contains a suggestion while a label of $0$ indicates that the text does not contain a suggestion.\n",
    "\n",
    "You can find more details about the dataset in Sections 1, 2, 3 and 4 of [SemEval-2019 Task 9: Suggestion Mining from Online Reviews and Forums\n",
    "](https://aclanthology.org/S19-2151/).\n",
    "\n",
    "We will be using test_seen.csv for benchmarking our model, hence it has label. On the other hand, test_unseen is used for [Kaggle](https://www.kaggle.com/competitions/nlp2022ct5120suggestionmining/overview) competition.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ShQ2lPxmPfA4",
    "outputId": "df651146-abe3-4d3b-8960-23eb1d2b977b"
   },
   "outputs": [],
   "source": [
    "# !curl \"https://raw.githubusercontent.com/sharduls007/Assignment_2_CT5120/master/train.csv\" > train.csv\n",
    "# !curl \"https://raw.githubusercontent.com/sharduls007/Assignment_2_CT5120/master/test_seen.csv\" > test.csv\n",
    "# !curl \"https://raw.githubusercontent.com/sharduls007/Assignment_2_CT5120/master/test_unseen.csv\" > test_unseen.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "glove = api.load('glove-wiki-gigaword-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "5x0c38rCGk23"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Read the CSV file.\n",
    "train_df = pd.read_csv('train.csv', \n",
    "                 names=['id', 'text', 'label'], header=0)\n",
    "\n",
    "test_df = pd.read_csv('test.csv', \n",
    "                 names=['id', 'text', 'label'], header=0)\n",
    "\n",
    "# Store the data as a list of tuples where the first item is the text\n",
    "# and the second item is the label.\n",
    "train_texts, train_labels = train_df[\"text\"].to_list(), train_df[\"label\"].to_list() \n",
    "test_texts, test_labels = test_df[\"text\"].to_list(), test_df[\"label\"].to_list() \n",
    "\n",
    "# Check that training set and test set are of the right size.\n",
    "assert len(test_texts) == len(test_labels) == 1360\n",
    "assert len(train_texts) == len(train_labels) == 5440"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I_Scj45oSpdQ"
   },
   "source": [
    "---\n",
    "\n",
    "## Task 1: Data Pre-processing (10 Marks)\n",
    "\n",
    "Explain at least 3 steps that you will perform to preprocess the texts before training a classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8Pd8ed8NdlB_"
   },
   "source": [
    "\n",
    "\n",
    "Edit this cell to write your answer below the line in no more than 300 words.\n",
    "\n",
    "---\n",
    "\n",
    "> Check if there is any missing value.\n",
    "\n",
    "> Remove the stop words (function words) which are less imformative about the text. We can safely remove them without lossing any valuable information.\n",
    "\n",
    "> Remove the punctuations. Punctuations can be less imformative about the text. Hence, can remove them.\n",
    "\n",
    "> Lowercase the text. It is one of the most common preprocessing steps where the text is converted into the lower case.\n",
    "\n",
    "> Stemming. Stemming is a simplified analysis of word structure by removing endings or beginnings of words and leaving a common stem. In the code below, I use PorterStemmer to keep stem of the word. \n",
    "\n",
    "> Lemmatisation. Lemmatisation is the linguistic analysis of word structure with a transformation to trnasform related words to a common lemma morphologically. In the code below, I use the WordNetLemmatizer to get the lemma of the word.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t2-xXQggaVKh"
   },
   "source": [
    "In the code cell below, write an implementation of the steps you defined above. You are free to use a library such as `nltk` or `sklearn` for this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def data_preprocessing(df):\n",
    "    \n",
    "    # Check if there is any missing value.\n",
    "    print(f\"Are there missing value? {df.isnull().any()}\")\n",
    "    \n",
    "    # remove stopwords\n",
    "    stops = set(stopwords.words('english')) # store the english stop words\n",
    "    df['preprocess'] = df['text'].apply(lambda x: ' '.join([w for w in x.split() if w not in (stops)]))\n",
    "    \n",
    "    # remove puncuations\n",
    "    punct = punctuation + '—«»–„“‘’'\n",
    "    df['preprocess'] = df['preprocess'].apply(lambda x: ' '.join([w.strip(punct) for w in x.split()]))\n",
    "\n",
    "    # lowercase the text\n",
    "    df['preprocess'] = df['preprocess'].apply(lambda x: ' '.join([w.lower() for w in x.split()]))\n",
    "    \n",
    "    # stemming\n",
    "    stemmer = PorterStemmer()\n",
    "    df[\"preprocess\"] = df[\"preprocess\"].apply(lambda x: ' '.join([stemmer.stem(w) for w in x.split()]))\n",
    "\n",
    "    # Lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    df[\"preprocess\"] = df[\"preprocess\"].apply(lambda x: ' '.join([lemmatizer.lemmatize(w) for w in x.split()]))\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Are there missing value? id            False\n",
      "text          False\n",
      "label         False\n",
      "preprocess    False\n",
      "dtype: bool\n",
      "Are there missing value? id            False\n",
      "text          False\n",
      "label         False\n",
      "preprocess    False\n",
      "dtype: bool\n"
     ]
    }
   ],
   "source": [
    "train_preprocess_df = data_preprocessing(train_df)\n",
    "test_preprocess_df = data_preprocessing(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>preprocess</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train_0</td>\n",
       "      <td>\"One would hope if I search for a word in the ...</td>\n",
       "      <td>0</td>\n",
       "      <td>one would hope i search word titl app would sh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>train_1</td>\n",
       "      <td>\"I would be beyond excited to get a response.\"</td>\n",
       "      <td>0</td>\n",
       "      <td>i would beyond excit get respons</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>train_2</td>\n",
       "      <td>\"Just like the user can select apps that are a...</td>\n",
       "      <td>1</td>\n",
       "      <td>just like user select app allow run background...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>train_3</td>\n",
       "      <td>\"Once you create a CoreIndependentInputSource ...</td>\n",
       "      <td>0</td>\n",
       "      <td>onc creat coreindependentinputsourc touch visu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>train_4</td>\n",
       "      <td>\"I Have problems with Contact class on Windows...</td>\n",
       "      <td>0</td>\n",
       "      <td>i have problem contact class window 8.1 window...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                               text  label  \\\n",
       "0  train_0  \"One would hope if I search for a word in the ...      0   \n",
       "1  train_1     \"I would be beyond excited to get a response.\"      0   \n",
       "2  train_2  \"Just like the user can select apps that are a...      1   \n",
       "3  train_3  \"Once you create a CoreIndependentInputSource ...      0   \n",
       "4  train_4  \"I Have problems with Contact class on Windows...      0   \n",
       "\n",
       "                                          preprocess  \n",
       "0  one would hope i search word titl app would sh...  \n",
       "1                   i would beyond excit get respons  \n",
       "2  just like user select app allow run background...  \n",
       "3  onc creat coreindependentinputsourc touch visu...  \n",
       "4  i have problem contact class window 8.1 window...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_preprocess_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>preprocess</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>dev_0</td>\n",
       "      <td>\"I understand why you might do this for the FR...</td>\n",
       "      <td>1</td>\n",
       "      <td>i understand might free version feedli implor ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dev_1</td>\n",
       "      <td>\"This is a significant bug and kind of hard to...</td>\n",
       "      <td>0</td>\n",
       "      <td>thi signific bug kind hard believ fix</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dev_2</td>\n",
       "      <td>\"This leads the user to have to type AGAIN the...</td>\n",
       "      <td>0</td>\n",
       "      <td>thi lead user type again whole descript</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dev_3</td>\n",
       "      <td>\"Needless to say, disappointed, it appears I c...</td>\n",
       "      <td>0</td>\n",
       "      <td>needle say disappoint appear i cannot develop ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>dev_4</td>\n",
       "      <td>\"Implementing the Auto-Upload feature in a Sil...</td>\n",
       "      <td>0</td>\n",
       "      <td>implement auto-upload featur silverlight 8.1 a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                                               text  label  \\\n",
       "0  dev_0  \"I understand why you might do this for the FR...      1   \n",
       "1  dev_1  \"This is a significant bug and kind of hard to...      0   \n",
       "2  dev_2  \"This leads the user to have to type AGAIN the...      0   \n",
       "3  dev_3  \"Needless to say, disappointed, it appears I c...      0   \n",
       "4  dev_4  \"Implementing the Auto-Upload feature in a Sil...      0   \n",
       "\n",
       "                                          preprocess  \n",
       "0  i understand might free version feedli implor ...  \n",
       "1              thi signific bug kind hard believ fix  \n",
       "2            thi lead user type again whole descript  \n",
       "3  needle say disappoint appear i cannot develop ...  \n",
       "4  implement auto-upload featur silverlight 8.1 a...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_preprocess_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.int64"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os = 0\n",
    "s = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3IUJunnfXItQ"
   },
   "source": [
    "---\n",
    "\n",
    "## Task 2: Feature Engineering (I) - TF-IDF as features (10 Marks)\n",
    "\n",
    "In the lectures we have seen that raw counts of words and `tf-idf` scores can be useful features for a classification task. Complete the following code cell to create a suggestion detector which uses `tf-idf` scores as features for a Naïve Bayes classifier.\n",
    "\n",
    "After applying your preprocessing steps, use the training data to train the classifier and make predictions on the test set. You **must not** use the test set for training.\n",
    "\n",
    "If everything is implemented correctly, then you should see a single floating point value between 0 and 1 at the end which denotes the accuracy of the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "3gDsfB8xTGMg",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5279411764705882"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# Calculate tf-idf scores for the words in the training set.\n",
    "# ... your code goes here\n",
    "\n",
    "count_vectorizer = CountVectorizer()\n",
    "train_X = train_df[\"preprocess\"]\n",
    "X_train_counts = count_vectorizer.fit_transform(x for x in train_X)\n",
    "\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "\n",
    "\n",
    "# Train a Naïve Bayes classifier using the tf-idf scores for words as features.\n",
    "# ... your code goes here\n",
    "NB_classifier_tfidf = GaussianNB()\n",
    "NB_classifier_tfidf.fit(X_train_tfidf.toarray(), train_labels)\n",
    "\n",
    "\n",
    "# Predict on the test set.\n",
    "predictions = []    # save your predictions on the test set into this list\n",
    "\n",
    "# ... your code goes here\n",
    "# generate the features for the test set\n",
    "test_X = test_df[\"preprocess\"]\n",
    "X_test_counts = count_vectorizer.transform(x for x in test_X)\n",
    "X_test_tfidf = tfidf_transformer.transform(X_test_counts)\n",
    "\n",
    "predictions = NB_classifier_tfidf.predict(X_test_tfidf.toarray())\n",
    "\n",
    "\n",
    "#################### DO NOT EDIT BELOW THIS LINE #################\n",
    "\n",
    "\n",
    "#################### DO NOT EDIT BELOW THIS LINE #################\n",
    "\n",
    "def accuracy(labels, predictions):\n",
    "    '''\n",
    "    Calculate the accuracy score for a given set of predictions and labels.\n",
    "\n",
    "    Args:\n",
    "    labels (list): A list containing gold standard labels annotated as `0` and `1`.\n",
    "    predictions (list): A list containing predictions annotated as `0` and `1`.\n",
    "\n",
    "    Returns:\n",
    "    float: A floating point value to score the predictions against the labels.\n",
    "    '''\n",
    "\n",
    "    assert len(labels) == len(predictions)\n",
    "\n",
    "    correct = 0\n",
    "    for label, prediction in zip(labels, predictions):\n",
    "        if label == prediction:\n",
    "            correct += 1 \n",
    "\n",
    "    score = correct / len(labels)\n",
    "    return score\n",
    "\n",
    "# Calculate accuracy score for the classifier using tf-idf features.\n",
    "accuracy(test_labels, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DDx_M2aTIncl"
   },
   "source": [
    "---\n",
    "\n",
    "## Task 3: Evaluation Metrics (10 marks)\n",
    "\n",
    "Why is accuracy not the best measure for evaluating a classifier? Describe an evaluation metric which might work better than accuracy for a classification task such as suggestion detection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M8jDzSU86xI1"
   },
   "source": [
    "Edit this cell to write your answer below the line in no more than 150 words.\n",
    "\n",
    "---\n",
    "\n",
    "> We formalise the accuracy as $Accuracy=\\frac{correct}{len(labels)}$. It will fail when the dataset consists of unbalanced classes. For instance, in one dataset, if we have 90% data belong to class A and 10% data belong to class B. Even if the hypothesis fails to predict the data belongs to class B, the accuracy is still high as it can predict the data belong to class A. Therefore, accuracy does not the best measure for evaluating a classifier. \n",
    "\n",
    "> Confusion matrix with f1-score measurement works better than accuracy in classification. Especially in imbalanced classification problems, f1-score measurement works better than accuracy as f1-score takes into account of balancing precision and recall on the positive class.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2ozD4SyyRDL3"
   },
   "source": [
    "In the code cell below, write an implementation of the evaluation metric you defined above. Please write your own implementation from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "id": "UkUX5K0oMhKI"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[499. 528.]\n",
      " [114. 219.]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6085365853658536"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def evaluate(labels, predictions):\n",
    "    '''\n",
    "    Calculate an evaluation score other than accuracy for a given set of predictions and labels.\n",
    "\n",
    "    Args:\n",
    "    labels (list): A list containing gold standard labels annotated as `0` and `1`.\n",
    "    predictions (list): A list containing predictions annotated as `0` and `1`.\n",
    "\n",
    "    Returns:\n",
    "    float: A floating point value to score the predictions against the labels.\n",
    "    '''\n",
    "\n",
    "    # check that labels and predictions are of same length\n",
    "    #assert len(labels) == len(predictions)\n",
    "\n",
    "    score = 0.0\n",
    "\n",
    "    #################### EDIT BELOW THIS LINE #########################\n",
    "\n",
    "    # your code goes here\n",
    "    num_classes = len(np.unique(labels)) # Number of classes \n",
    "    c_matrix = np.zeros((num_classes, num_classes))\n",
    "    \n",
    "    for i in range(len(labels)):\n",
    "        c_matrix[labels[i]][predictions[i]] += 1\n",
    "\n",
    "    print(c_matrix)\n",
    "\n",
    "    score = 2*c_matrix[0][0] / (2*c_matrix[0][0] + c_matrix[1][0] + c_matrix[0][1]) # F1 score\n",
    "    \n",
    "    #################### EDIT ABOVE THIS LINE #########################\n",
    "\n",
    "    return score\n",
    "\n",
    "# Calculate evaluation score based on the metric of your choice\n",
    "# for the classifier trained in Task 2 using tf-idf features.\n",
    "evaluate(test_labels, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "22OelF89a27J"
   },
   "source": [
    "---\n",
    "\n",
    "## Task 4: Feature Engineering (II) - Other features (10 Marks)\n",
    "\n",
    "Describe features other than those defined in Task 2 which might improve the performance of your suggestion detector. If these features require any additional pre-processing steps, then define those steps as well.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4EBS0F877UyC"
   },
   "source": [
    "Edit this cell to write your answer below the line in no more than 500 words.\n",
    "\n",
    "---\n",
    "\n",
    "> Delete this line and your write answer here\n",
    "\n",
    "> Using Word Embeddings as Features\n",
    "\n",
    "> GloVe is very similar to word2vec, but it starts off with buildings a co-occurence matrix, which makes it a hybrid model.\n",
    "GloVe is based on ratios of probabilities from the word-word co-occurrence matrix, combining the intuitions of count-based models while also capturing the linear structures used by methods like word2vec (Pennington et al. 2014)\n",
    "Here and here are good explanations of the differences between word2vec and GloVe in more detail.\n",
    "The GloVe embedding model was proposed by the Stanford NLP group in 2014. Here is the article and the website for the project.\n",
    "\n",
    "> Tokenization. Tokenization is the process of dividing a text into smaller units i.e. tokens.\n",
    "nltk provides a word_tokenize function to tokenize a sentence in a given language.\n",
    "\n",
    "> mean pooling\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yfkzM3DRce14"
   },
   "source": [
    "In the code cell below, write an implementation of the features (and any additional pre-preprocessing steps) you defined above. You are free to use a library such as `nltk` or `sklearn` for this task.\n",
    "\n",
    "After creating your features, use the training data to train a Naïve Bayes classifier and use the test set to evaluate its performance using the metric defined in Task 3. You **must not** use the test set for training.\n",
    "\n",
    "To make sure that your code doesn't take too long to run or use too much memory, you can consider a time limit of 3 minutes and a memory limit of 12GB for this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tokens = [word_tokenize(sent) for sent in train_df[\"text\"]]\n",
    "test_tokens = [word_tokenize(sent) for sent in test_df[\"text\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "all_text_vecs = []\n",
    "oov = np.random.rand(1,300) # random vector to represent out-of-vocab\n",
    "\n",
    "# 1. Loop over tokenized texts \n",
    "for toks in train_tokens:\n",
    "    text_vecs = []\n",
    "\n",
    "    # 2. Loop over each word in the tokenzed input\n",
    "    for tok in toks:\n",
    "        # If token exists in GloVe, add its embedding to text_vecs\n",
    "        if tok in glove:\n",
    "            text_vecs.append(glove[tok])\n",
    "        # Special case where no word embedding is found\n",
    "        else:\n",
    "            text_vecs.append(oov)\n",
    "  \n",
    "    # 3. Once you extracted all the word embeddings for\n",
    "    # the input text, append text_vecs to all_text_vecs\n",
    "    all_text_vecs.append(text_vecs)\n",
    "\n",
    "assert len(all_text_vecs) == len(train_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity check: True\n",
      "Sanity check: True\n"
     ]
    }
   ],
   "source": [
    "all_pooled_vecs = []\n",
    "\n",
    "# 1. Loop over each list of word embeddings per input\n",
    "for text_vecs in all_text_vecs:\n",
    "    # 2. Vstack and take the mean of the tex_vecs\n",
    "    mean_pool = np.mean(np.vstack(text_vecs), axis=0)\n",
    "    # 3. Append the mean pooled vector to all_pooled_vecs\n",
    "    all_pooled_vecs.append(mean_pool)\n",
    "\n",
    "# 4. Update dataset with these pooled vectors\n",
    "#dataset[\"embeddings\"] = all_pooled_vecs\n",
    "\n",
    "print(f\"Sanity check: {len(all_pooled_vecs) == len(all_text_vecs)}\")\n",
    "print(f\"Sanity check: {np.vstack(all_pooled_vecs).shape[1] == 300}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "id": "u9mRku0va8kK"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== time elapse: 0.5326499938964844 =========\n",
      "[[699. 328.]\n",
      " [219. 114.]]\n",
      "score: 0.7187660668380462\n"
     ]
    }
   ],
   "source": [
    "# Create your features.\n",
    "# ... your code goes here\n",
    "import time\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# Tokenization\n",
    "train_tokens = [word_tokenize(sent) for sent in train_df[\"preprocess\"]]\n",
    "test_tokens = [word_tokenize(sent) for sent in test_df[\"preprocess\"]]\n",
    "# train_tokens = [word_tokenize(sent) for sent in train_df[\"text\"]]\n",
    "# test_tokens = [word_tokenize(sent) for sent in test_df[\"text\"]]\n",
    "\n",
    "# def word_embedding(tokens):\n",
    "#     all_text_vecs = []\n",
    "#     text_vecs = []\n",
    "#     all_pooled_vecs = []\n",
    "    \n",
    "#     oov = np.random.rand(1,300) # random vector to represent out-of-vocab\n",
    "\n",
    "#     for toks in tokens:\n",
    "#         for tok in toks:\n",
    "#             if tok in glove:\n",
    "#                 text_vecs.append(glove[tok])\n",
    "#             else:\n",
    "#                 text_vecs.append(oov)\n",
    "#         all_text_vecs.append(text_vecs)\n",
    "\n",
    "#     #assert len(all_text_vecs) == len(train_tokens)\n",
    "    \n",
    "#     for text_vecs in all_text_vecs:\n",
    "#         mean_pool = np.mean(np.vstack(text_vecs), axis=0)\n",
    "#         all_pooled_vecs.append(mean_pool)\n",
    "#     return all_pooled_vecs\n",
    "\n",
    "\n",
    "def word_embedding(tokens):\n",
    "    all_text_vecs = []\n",
    "    oov = np.random.rand(1,300) # random vector to represent out-of-vocab\n",
    "\n",
    "    # 1. Loop over tokenized texts \n",
    "    for toks in train_tokens:\n",
    "        text_vecs = []\n",
    "\n",
    "        # 2. Loop over each word in the tokenzed input\n",
    "        for tok in toks:\n",
    "            # If token exists in GloVe, add its embedding to text_vecs\n",
    "            if tok in glove:\n",
    "                text_vecs.append(glove[tok])\n",
    "            # Special case where no word embedding is found\n",
    "            else:\n",
    "                text_vecs.append(oov)\n",
    "        \n",
    "        if len(text_vecs) == 0: text_vecs = oov\n",
    "        # 3. Once you extracted all the word embeddings for\n",
    "        # the input text, append text_vecs to all_text_vecs\n",
    "        all_text_vecs.append(text_vecs)\n",
    "\n",
    "    all_pooled_vecs = []\n",
    "\n",
    "    # 1. Loop over each list of word embeddings per input\n",
    "    for text_vec in all_text_vecs:\n",
    "        # 2. Vstack and take the mean of the tex_vecs\n",
    "        mean_pool = np.mean(np.vstack(text_vec), axis=0)\n",
    "        # 3. Append the mean pooled vector to all_pooled_vecs\n",
    "        all_pooled_vecs.append(mean_pool)\n",
    "\n",
    "    return all_pooled_vecs\n",
    "\n",
    "\n",
    "\n",
    "# Train a Naïve Bayes classifier using the features you defined.\n",
    "# ... your code goes here\n",
    "NB_classifier_emb = GaussianNB()\n",
    "NB_classifier_emb.fit(word_embedding(train_tokens), train_labels)\n",
    "preds = NB_classifier_emb.predict(word_embedding(test_tokens))\n",
    "\n",
    "print(f\"======== time elapse: {time.time() - start} =========\")\n",
    "# Evaluate on the test set.\n",
    "# ... your code goes here\n",
    "score = evaluate(test_labels, preds)\n",
    "print(f\"score: {score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yyDD1zFQdwCf"
   },
   "source": [
    "---\n",
    "\n",
    "## Task 5: Kaggle Competition (10 marks)\n",
    "\n",
    "Head over to https://www.kaggle.com/t/1f90b74da0b7484da9647638e22d1068  \n",
    "Use above classifier to predict the label for test_unseen.csv from competition page and upload the results to the leaderboard. The current baseline score is 0.36823. Make an improvement above the baseline. Please note that the evaluation metric for the competition is the f-score.\n",
    "\n",
    "Read competition page for more details.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "D9NZrBayoN4A",
    "outputId": "d2c338a4-f20f-429e-9c69-a4a7850de428",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "JaC6B824Fe0H"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Are there missing value? id      False\n",
      "text    False\n",
      "dtype: bool\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [56], line 8\u001b[0m\n\u001b[1;32m      3\u001b[0m test_unseen \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_unseen.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, names\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m], header\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Here Id is unique identifier assigned to each test sample ranging from test_0 till test_1699\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Expected is a list of prediction made by your classifier\u001b[39;00m\n\u001b[1;32m      7\u001b[0m sub \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mId\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(test_unseen))],\n\u001b[0;32m----> 8\u001b[0m        \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected\u001b[39m\u001b[38;5;124m\"\u001b[39m: [x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m NB_classifier_emb\u001b[38;5;241m.\u001b[39mpredict(\u001b[43mword_embedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mword_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43msent\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msent\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdata_preprocessing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_unseen\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpreprocess\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m)]}\n\u001b[1;32m     10\u001b[0m sub_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(sub)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# The code below will generate a StudentID.csv on your drive on the left hand side in the explorer\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Please upload the file as a submission on the competition page\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# You can index your submission StudentID_Lastname_index.csv, where index is your number of submission\u001b[39;00m\n",
      "Cell \u001b[0;32mIn [38], line 30\u001b[0m, in \u001b[0;36mword_embedding\u001b[0;34m(tokens)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m#assert len(all_text_vecs) == len(train_tokens)\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m text_vecs \u001b[38;5;129;01min\u001b[39;00m all_text_vecs:\n\u001b[0;32m---> 30\u001b[0m     mean_pool \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext_vecs\u001b[49m\u001b[43m)\u001b[49m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     31\u001b[0m     all_pooled_vecs\u001b[38;5;241m.\u001b[39mappend(mean_pool)\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m all_pooled_vecs\n",
      "File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mvstack\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/research/lib/python3.9/site-packages/numpy/core/shape_base.py:279\u001b[0m, in \u001b[0;36mvstack\u001b[0;34m(tup)\u001b[0m\n\u001b[1;32m    276\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m overrides\u001b[38;5;241m.\u001b[39mARRAY_FUNCTION_ENABLED:\n\u001b[1;32m    277\u001b[0m     \u001b[38;5;66;03m# raise warning if necessary\u001b[39;00m\n\u001b[1;32m    278\u001b[0m     _arrays_for_stack_dispatcher(tup, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m--> 279\u001b[0m arrs \u001b[38;5;241m=\u001b[39m \u001b[43matleast_2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtup\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    280\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arrs, \u001b[38;5;28mlist\u001b[39m):\n\u001b[1;32m    281\u001b[0m     arrs \u001b[38;5;241m=\u001b[39m [arrs]\n",
      "File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36matleast_2d\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/research/lib/python3.9/site-packages/numpy/core/shape_base.py:121\u001b[0m, in \u001b[0;36matleast_2d\u001b[0;34m(*arys)\u001b[0m\n\u001b[1;32m    119\u001b[0m res \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ary \u001b[38;5;129;01min\u001b[39;00m arys:\n\u001b[0;32m--> 121\u001b[0m     ary \u001b[38;5;241m=\u001b[39m \u001b[43masanyarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mary\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    122\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ary\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    123\u001b[0m         result \u001b[38;5;241m=\u001b[39m ary\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Preparing submission for Kaggle\n",
    "StudentID = \"20230033_Li\" # Please add your student id and lastname\n",
    "test_unseen = pd.read_csv(\"test_unseen.csv\", names=['id', 'text'], header=0)\n",
    "\n",
    "# Here Id is unique identifier assigned to each test sample ranging from test_0 till test_1699\n",
    "# Expected is a list of prediction made by your classifier\n",
    "sub = {\"Id\": [f\"test_{i}\" for i in range(len(test_unseen))],\n",
    "       \"Expected\": [x for x in NB_classifier_emb.predict(word_embedding([word_tokenize(sent) for sent in data_preprocessing(test_unseen)[\"preprocess\"]]))]}\n",
    "\n",
    "sub_df = pd.DataFrame(sub)\n",
    "# The code below will generate a StudentID.csv on your drive on the left hand side in the explorer\n",
    "# Please upload the file as a submission on the competition page\n",
    "# You can index your submission StudentID_Lastname_index.csv, where index is your number of submission\n",
    "sub_df.to_csv(f\"{StudentID}.csv\", sep=\",\", header=1, index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(sub_df[\"Expected\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a6brptmkqY9C"
   },
   "source": [
    "Mention the approach that you have chosen briefly, and what is the mean average f-score that you have achieved? Did it improve above the chosen baseline model (0.36823)? Why or why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GZQumdT-9yet"
   },
   "source": [
    "Edit this cell to write your answer below the line in no more than 500 words.\n",
    "\n",
    "---\n",
    "\n",
    "> Delete this line and your write answer here\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
