{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b2f6e33",
   "metadata": {},
   "source": [
    "# Programming and Tools for AI (/Online)\n",
    "# James McDermott, University of Galway, 2022\n",
    "# CT5132/CT5148\n",
    "# Assignment 2\n",
    "\n",
    "**Due**:\n",
    "\n",
    "* CT5132 midnight Friday 28 Oct (end of Week 8)\n",
    "* CT5148 midnight Sunday 30 Oct (end of Week 8)\n",
    "\n",
    "**Weight**: this is worth 25% of the module.\n",
    "\n",
    "**Groups**: you can work solo or in a group of two, as you prefer. You can not work together with another student you already worked with in any assignment in this module or any other. If working in a group, both students should submit and their submissions should be identical.\n",
    "\n",
    "**Plagiarism**: students are reminded of the University's policies on plagiarism. Code copied from the internet should be sourced with the URL in a comment. Students can discuss the assignment but must not look at each other's work (other than within a group). Some cases were reported to relevant authorities during Assignment 1.\n",
    "\n",
    "**Requirements**:\n",
    "\n",
    "* Add your name(s)/ID(s) in the next cell.\n",
    "* Write code in this notebook to pass the doctests and produce the images as directed.\n",
    "* Write new doctests. For each docstring below which contains doctests, you have to add one more doctest inside the same docstring, together with a line or two of explanation in your own words.\n",
    "* Submit your .ipynb file and nothing else. Don't type in the submission box.\n",
    "\n",
    "**Notes**:\n",
    "\n",
    "Do not change or delete any doctests, or insert lines above the docstrings, or change the doctest calls.\n",
    "\n",
    "When I receive your submission, I'll go to the Kernel menu and \"Restart and run all\", and I'll look at your code, your doctests, and your outputs. So, the last thing you should do before submitting is \"Restart and run all\" and check that every part runs correctly. (One thing that sometimes goes wrong in notebooks is that a piece of code relies on a variable or function which was previously defined but is now deleted but is still \"alive\" in the kernel's memory, so the notebook crashes when re-run.)\n",
    "\n",
    "Marks will be awarded for each of the 12 questions. Both correct outputs and code will be assessed. Images are not expected to match exactly, but doctests are.\n",
    "\n",
    "If you get stuck on any part, you should still be able to continue to other parts. If you can't see how to proceed in this case, please ask me.\n",
    "\n",
    "There are a lot of parts, so even if you don't get everything, you can still get a good mark."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b92c090",
   "metadata": {},
   "source": [
    "**Student ID(s)**: 20230033\n",
    "\n",
    "**Name(s)**: Jiarong Li\n",
    "\n",
    "**Declaration**: By submitting to Blackboard, I/we declare that I/we have not seen any work by other students on this assignment and have not shown my/our work to any others. I/we declare that we have not worked together on any previous assignment in this module or another."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c7ad993",
   "metadata": {},
   "source": [
    "# Clustering Sequences using $n$-grams\n",
    "\n",
    "In this assignment, we'll put together a clustering method specialised to sequence mining, in the style of Scikit-Learn. We'll use components from Scikit-Learn itself, Numpy, and Matplotlib.\n",
    "\n",
    "Usually when we do machine learning, we fit with numerical, rectangular data, so each sample is a feature vector. But sequences are a bit different. For a start, each sequence might be of different length. Also, commonalities between sequences might be mis-aligned, eg these two sequences are pretty similar:\n",
    "\n",
    "$(0, 0, 1, 4, 7, 0)$\n",
    "\n",
    "$(0, 1, 4, 7, 0, 1)$\n",
    "\n",
    "The approach we'll use is to count common subsequences of length $n$, ie $n$-grams. \n",
    "\n",
    "Many machine learning algorithms work fine even if we don't have feature vectors, but we have distances between points. \n",
    "\n",
    "We will use the $n$-gram counts to compute a measure of dissimilarity between any pair of sequences.\n",
    "\n",
    "In Scikit-Learn, several algorithms accept a keyword such as `metric=\"precomputed\"` or `affinity=\"precomputed\"`, which allows us to pass in the square matrix of distances instead of passing in the points themselves.\n",
    "\n",
    "This notebook will walk us through all the steps, with doctests and examples for each. Most of the code needed is based on something we've seen in class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1daac734",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import sys\n",
    "assert sys.version_info >= (3, 7), \"This notebook requires Python 3.7+\"\n",
    "\n",
    "from collections import Counter\n",
    "import random\n",
    "import doctest\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.base import BaseEstimator, ClusterMixin\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import AgglomerativeClustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "7da69160",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 1, 2, 0, 1, 2, 1, 0, 2, 0, 2, 2, 0, 1, 0, 0, 2, 0, 2, 0, 0, 0, 0, 1, 1, 1, 0, 2, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 2]\n",
      "[3, 3, 5, 3, 4, 3, 5, 1, 5, 5, 6, 5, 1, 4, 3, 4, 2, 3, 4, 3, 5, 3, 5, 1, 5, 3, 3, 4, 3, 5, 6, 2, 2, 4, 6, 6, 4, 6, 3, 2]\n",
      "[4, 2, 3, 2, 3, 4, 2, 2, 4, 4, 4, 2, 2, 3, 4, 2, 3, 4, 4, 3, 4, 3, 4, 2, 4, 4, 2, 4, 4, 2]\n"
     ]
    }
   ],
   "source": [
    "# here we construct 10 fake sequences from each of 3 methods\n",
    "X = [[random.randrange(0, 3) for _ in range(40)] for _ in range(50)] + \\\n",
    "    [[random.randrange(1, 7) for _ in range(40)] for _ in range(50)] + \\\n",
    "    [[random.randrange(2, 5) for _ in range(30)] for _ in range(50)]\n",
    "print(X[0]) # take a look at one example from each method\n",
    "print(X[50])\n",
    "print(X[100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e19fe7b6",
   "metadata": {},
   "source": [
    "**Question 1**. Suppose we have a sequence of integers, `x`, eg `x = [0, 1, 4, 0, 1, 4, 7, 0, 1, 4]`. We immediately see that there are some common subsequences: eg `[0, 1, 4]` occurs three times.\n",
    "\n",
    "Write a function `count_ngrams` which extracts all *n-grams*, ie all subsequences of length `n`, and returns their number of occurrences in a `Counter`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d2dd930f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_ngrams(x, n):\n",
    "    '''\n",
    "The input can be a list:\n",
    ">>> count_ngrams([0, 1, 4, 0, 1, 4, 7, 0, 1, 4], 3)\n",
    "Counter({(0, 1, 4): 3,\n",
    "         (1, 4, 0): 1,\n",
    "         (4, 0, 1): 1,\n",
    "         (1, 4, 7): 1,\n",
    "         (4, 7, 0): 1,\n",
    "         (7, 0, 1): 1})\n",
    "         \n",
    "It can be a tuple:\n",
    ">>> count_ngrams((0, 1, 4, 0, 1, 4, 7, 0, 1, 4), 5)\n",
    "Counter({(0, 1, 4, 0, 1): 1, \n",
    "         (1, 4, 0, 1, 4): 1, \n",
    "         (4, 0, 1, 4, 7): 1, \n",
    "         (0, 1, 4, 7, 0): 1, \n",
    "         (1, 4, 7, 0, 1): 1, \n",
    "         (4, 7, 0, 1, 4): 1})\n",
    "         \n",
    "It can be empty:         \n",
    ">>> count_ngrams([], 3)\n",
    "Counter()\n",
    "\n",
    "It can even be a string:\n",
    ">>> count_ngrams(\"abcdabcdefacd\", 3)\n",
    "Counter({('a', 'b', 'c'): 2, ('b', 'c', 'd'): 2, ('c', 'd', 'a'): 1, \n",
    "         ('d', 'a', 'b'): 1, ('c', 'd', 'e'): 1, ('d', 'e', 'f'): 1, \n",
    "         ('e', 'f', 'a'): 1, ('f', 'a', 'c'): 1, ('a', 'c', 'd'): 1})\n",
    "         \n",
    "The input can be a string:\n",
    ">>> count_ngrams(\"asd345fgh678\", 3)\n",
    "Counter({('a', 's', 'd'): 1, ('s', 'd', '3'): 1, ('d', '3', '4'): 1,\n",
    "         ('3', '4', '5'): 1, ('4', '5', 'f'): 1, ('5', 'f', 'g'): 1,\n",
    "         ('f', 'g', 'h'): 1, ('g', 'h', '6'): 1, ('h', '6', '7'): 1,\n",
    "         ('6', '7', '8'): 1})\n",
    "    '''\n",
    "    # YOUR CODE HERE\n",
    "    # my code is 1 line\n",
    "    subsequences = []    \n",
    "    for i in list(range(len(x) - n + 1)):\n",
    "        subsequences.append(x[i:i+n])\n",
    "\n",
    "    return Counter(map(tuple, subsequences)) # map() here convert list object to a tuple \n",
    "                                             # and return a literable map object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6d694d83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding tests in NoName\n",
      "Trying:\n",
      "    count_ngrams([0, 1, 4, 0, 1, 4, 7, 0, 1, 4], 3)\n",
      "Expecting:\n",
      "    Counter({(0, 1, 4): 3,\n",
      "             (1, 4, 0): 1,\n",
      "             (4, 0, 1): 1,\n",
      "             (1, 4, 7): 1,\n",
      "             (4, 7, 0): 1,\n",
      "             (7, 0, 1): 1})\n",
      "ok\n",
      "Trying:\n",
      "    count_ngrams((0, 1, 4, 0, 1, 4, 7, 0, 1, 4), 5)\n",
      "Expecting:\n",
      "    Counter({(0, 1, 4, 0, 1): 1, \n",
      "             (1, 4, 0, 1, 4): 1, \n",
      "             (4, 0, 1, 4, 7): 1, \n",
      "             (0, 1, 4, 7, 0): 1, \n",
      "             (1, 4, 7, 0, 1): 1, \n",
      "             (4, 7, 0, 1, 4): 1})\n",
      "ok\n",
      "Trying:\n",
      "    count_ngrams([], 3)\n",
      "Expecting:\n",
      "    Counter()\n",
      "ok\n",
      "Trying:\n",
      "    count_ngrams(\"abcdabcdefacd\", 3)\n",
      "Expecting:\n",
      "    Counter({('a', 'b', 'c'): 2, ('b', 'c', 'd'): 2, ('c', 'd', 'a'): 1, \n",
      "             ('d', 'a', 'b'): 1, ('c', 'd', 'e'): 1, ('d', 'e', 'f'): 1, \n",
      "             ('e', 'f', 'a'): 1, ('f', 'a', 'c'): 1, ('a', 'c', 'd'): 1})\n",
      "ok\n",
      "Trying:\n",
      "    count_ngrams(\"asd345fgh678\", 3)\n",
      "Expecting:\n",
      "    Counter({('a', 's', 'd'): 1, ('s', 'd', '3'): 1, ('d', '3', '4'): 1,\n",
      "             ('3', '4', '5'): 1, ('4', '5', 'f'): 1, ('5', 'f', 'g'): 1,\n",
      "             ('f', 'g', 'h'): 1, ('g', 'h', '6'): 1, ('h', '6', '7'): 1,\n",
      "             ('6', '7', '8'): 1})\n",
      "ok\n"
     ]
    }
   ],
   "source": [
    "doctest.run_docstring_examples(count_ngrams, globals(), verbose=True, optionflags=doctest.NORMALIZE_WHITESPACE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7878028a",
   "metadata": {},
   "source": [
    "**Question 2**. Suppose we have a list of sequences, `X`. Write a function which gets the ngrams for each sequence in `X`. Of course it should just use our `count_ngrams`. Notice that `X` could be *ragged*, ie not every sequence in `X` has to be the same length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d653df7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_ngrams_multi(X, n):\n",
    "    '''\n",
    ">>> count_ngrams_multi([[0, 1, 4, 0, 1, 4, 7, 0, 1, 4],\n",
    "...                     [4, 5, 6, 4, 5, 6],\n",
    "...                     [1, 5, 9, 5, 1]], 3)\n",
    "[Counter({(0, 1, 4): 3, (1, 4, 0): 1, (4, 0, 1): 1, \n",
    "          (1, 4, 7): 1, (4, 7, 0): 1, (7, 0, 1): 1}), \n",
    " Counter({(4, 5, 6): 2, (5, 6, 4): 1, (6, 4, 5): 1}), \n",
    " Counter({(1, 5, 9): 1, (5, 9, 5): 1, (9, 5, 1): 1})]\n",
    ">>> count_ngrams_multi([[]], 3)\n",
    "[Counter()]\n",
    "\n",
    "Some sequences in X can be different length and can be a string and a list\n",
    ">>> count_ngrams_multi([\"abcdabcdefacd\",\n",
    "...                     [4, 5, 6, 4, 5, 6],\n",
    "...                     [1, 5, 9, 5, 1]], 3)\n",
    "[Counter({('a', 'b', 'c'): 2,\n",
    "          ('b', 'c', 'd'): 2,\n",
    "          ('c', 'd', 'a'): 1,\n",
    "          ('d', 'a', 'b'): 1,\n",
    "          ('c', 'd', 'e'): 1,\n",
    "          ('d', 'e', 'f'): 1,\n",
    "          ('e', 'f', 'a'): 1,\n",
    "          ('f', 'a', 'c'): 1,\n",
    "          ('a', 'c', 'd'): 1}),\n",
    " Counter({(4, 5, 6): 2, (5, 6, 4): 1, (6, 4, 5): 1}),\n",
    " Counter({(1, 5, 9): 1, (5, 9, 5): 1, (9, 5, 1): 1})]\n",
    "    '''\n",
    "    # YOUR CODE HERE\n",
    "    # my code is 1 line\n",
    "    return [count_ngrams(x, n) for x in X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "51ff1c86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding tests in NoName\n",
      "Trying:\n",
      "    count_ngrams_multi([[0, 1, 4, 0, 1, 4, 7, 0, 1, 4],\n",
      "                        [4, 5, 6, 4, 5, 6],\n",
      "                        [1, 5, 9, 5, 1]], 3)\n",
      "Expecting:\n",
      "    [Counter({(0, 1, 4): 3, (1, 4, 0): 1, (4, 0, 1): 1, \n",
      "              (1, 4, 7): 1, (4, 7, 0): 1, (7, 0, 1): 1}), \n",
      "     Counter({(4, 5, 6): 2, (5, 6, 4): 1, (6, 4, 5): 1}), \n",
      "     Counter({(1, 5, 9): 1, (5, 9, 5): 1, (9, 5, 1): 1})]\n",
      "ok\n",
      "Trying:\n",
      "    count_ngrams_multi([[]], 3)\n",
      "Expecting:\n",
      "    [Counter()]\n",
      "ok\n",
      "Trying:\n",
      "    count_ngrams_multi([\"abcdabcdefacd\",\n",
      "                        [4, 5, 6, 4, 5, 6],\n",
      "                        [1, 5, 9, 5, 1]], 3)\n",
      "Expecting:\n",
      "    [Counter({('a', 'b', 'c'): 2,\n",
      "              ('b', 'c', 'd'): 2,\n",
      "              ('c', 'd', 'a'): 1,\n",
      "              ('d', 'a', 'b'): 1,\n",
      "              ('c', 'd', 'e'): 1,\n",
      "              ('d', 'e', 'f'): 1,\n",
      "              ('e', 'f', 'a'): 1,\n",
      "              ('f', 'a', 'c'): 1,\n",
      "              ('a', 'c', 'd'): 1}),\n",
      "     Counter({(4, 5, 6): 2, (5, 6, 4): 1, (6, 4, 5): 1}),\n",
      "     Counter({(1, 5, 9): 1, (5, 9, 5): 1, (9, 5, 1): 1})]\n",
      "ok\n"
     ]
    }
   ],
   "source": [
    "# This runs our count_ngrams_multi doctests.\n",
    "doctest.run_docstring_examples(count_ngrams_multi, globals(), verbose=True, optionflags=doctest.NORMALIZE_WHITESPACE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f10bfa93",
   "metadata": {},
   "source": [
    "**Question 3**. We're going to need a helper function `counter_total` which measures the \"total size\" of a `Counter` by counting the total of its individual counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "69faaaeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def counter_total(c):\n",
    "    '''\n",
    "    Sum of the counts in a counter c\n",
    "    \n",
    "    If you have Python 3.10+, this is available directly as c.total().\n",
    "    \n",
    "    >>> counter_total(Counter())\n",
    "    0\n",
    "    >>> counter_total(Counter({'a': 1, 'b': 3}))\n",
    "    4\n",
    "    \n",
    "    The input can be a counter with multiple dicts\n",
    "    >>> counter_total(Counter({(0, 1, 4): 3, (1, 4, 0): 1, (4, 0, 1): 1, (1, 4, 7): 1, (4, 7, 0): 1, (7, 0, 1): 1}))\n",
    "    8\n",
    "    '''\n",
    "    # YOUR CODE HERE\n",
    "    # my code is 1 line\n",
    "    return sum(c.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2c513f74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding tests in NoName\n",
      "Trying:\n",
      "    counter_total(Counter())\n",
      "Expecting:\n",
      "    0\n",
      "ok\n",
      "Trying:\n",
      "    counter_total(Counter({'a': 1, 'b': 3}))\n",
      "Expecting:\n",
      "    4\n",
      "ok\n",
      "Trying:\n",
      "    counter_total(Counter({(0, 1, 4): 3, (1, 4, 0): 1, (4, 0, 1): 1, (1, 4, 7): 1, (4, 7, 0): 1, (7, 0, 1): 1}))\n",
      "Expecting:\n",
      "    8\n",
      "ok\n"
     ]
    }
   ],
   "source": [
    "doctest.run_docstring_examples(counter_total, globals(), verbose=True, optionflags=doctest.NORMALIZE_WHITESPACE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdbac596",
   "metadata": {},
   "source": [
    "**Question 4**.\n",
    "\n",
    "Next, we want to measure the *dissimilarity* between two sequences, `X` and `Y`.  \n",
    "\n",
    "Since we already have a method for representing a sequence as a `Counter` of n-grams, we only need a method for measuring the dissimilarity of two `Counter`s, $A$ and $B$. One main approach is the *Jaccard dissimilarity*. It's defined as:\n",
    "\n",
    "$$\\mathrm{JD}(A, B) = 1 - \\frac{|A \\cap B|}{|A \\cup B|}$$\n",
    "\n",
    "(Source: https://en.wikipedia.org/wiki/Jaccard_index.)\n",
    "\n",
    "In other words, $\\mathrm{JD}$ is low (dissimilarity low => similarity high) if there are many $n$-grams in common between `X` and `Y`. \n",
    "\n",
    "By the way, if you Google for \"python jaccard distance\" you'll find a lot of results which are not what we want, so be careful.\n",
    "\n",
    "To calculate intersection and union on `Counter` objects we can use `&` and `|` operators, and the result is a new `Counter`. After that we can use our `counter_total` to get the size `| |`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8f9abd86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def JD(A, B):\n",
    "    '''Jaccard dissimilarity on multisets, represented by Counters.\n",
    "    \n",
    "    Two are identical:\n",
    "    >>> JD(Counter({0, 1, 2, 3}), Counter({0, 1, 2, 3}))\n",
    "    0.0\n",
    "    \n",
    "    Two are similar:\n",
    "    >>> JD(Counter([0, 0, 0, 1, 1, 2, 3]), Counter([0, 0, 0, 1, 1, 2, 4]))\n",
    "    0.25\n",
    "    \n",
    "    Two are totally different:\n",
    "    >>> JD(Counter({0, 1, 2, 3}), Counter({10, 11, 12, 13}))\n",
    "    1.0\n",
    "    \n",
    "    Two are in different length:\n",
    "    >>> JD(Counter([0, 0, 0]), Counter([0, 0, 0, 1, 1, 2, 4]))\n",
    "    0.5714285714285714\n",
    "    '''\n",
    "    # YOUR CODE HERE\n",
    "    # my code is 1 line\n",
    "    \n",
    "    return (1 - (counter_total(A & B) / counter_total(A | B)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e2c37878",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding tests in NoName\n",
      "Trying:\n",
      "    JD(Counter({0, 1, 2, 3}), Counter({0, 1, 2, 3}))\n",
      "Expecting:\n",
      "    0.0\n",
      "ok\n",
      "Trying:\n",
      "    JD(Counter([0, 0, 0, 1, 1, 2, 3]), Counter([0, 0, 0, 1, 1, 2, 4]))\n",
      "Expecting:\n",
      "    0.25\n",
      "ok\n",
      "Trying:\n",
      "    JD(Counter({0, 1, 2, 3}), Counter({10, 11, 12, 13}))\n",
      "Expecting:\n",
      "    1.0\n",
      "ok\n",
      "Trying:\n",
      "    JD(Counter([0, 0, 0]), Counter([0, 0, 0, 1, 1, 2, 4]))\n",
      "Expecting:\n",
      "    0.5714285714285714\n",
      "ok\n"
     ]
    }
   ],
   "source": [
    "doctest.run_docstring_examples(JD, globals(), verbose=True, optionflags=doctest.NORMALIZE_WHITESPACE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4028ecb7",
   "metadata": {},
   "source": [
    "**Question 5**. As explained earlier, we need a function to precompute the distances between pairs of sequences and store them in an array. The main use of this is to calculate the pairwise distances between all pairs of n-grams derived from the training data (which is a list of sequences). The result of this is a square distance matrix, which will have zero on the diagonal. \n",
    "\n",
    "However, we can make our function slightly more general. If we write it to accept *two* lists of ngrams, and return a rectangular distance matrix then it will be useful in more situations. We can still use it for the original purpose by passing the same list twice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0172b7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance_matrix(ngrams1, ngrams2):\n",
    "    \"\"\"\n",
    "    If ngrams1 and ngrams2 are the same we get a square matrix:\n",
    "    >>> distance_matrix(count_ngrams_multi(['abcd', 'abc', 'acde'], 2), \n",
    "    ...                 count_ngrams_multi(['abcd', 'abc', 'acde'], 2)).shape == (3, 3)\n",
    "    True\n",
    "\n",
    "    And we get zeros on the diagonal, because JD(A, A) == 0 for any A:\n",
    "    >>> distance_matrix(count_ngrams_multi(['abcd', 'abc', 'acde'], 2), \n",
    "    ...                 count_ngrams_multi(['abcd', 'abc', 'acde'], 2)).diagonal()\n",
    "    array([0., 0., 0.])\n",
    "\n",
    "    If ngrams1 and ngrams2 are different lengths we get a rectangular (not square) matrix:\n",
    "    >>> distance_matrix(count_ngrams_multi(['abcd', 'abc', 'acde'], 2), \n",
    "    ...                 count_ngrams_multi(['abcd', 'abc'        ], 2)).shape == (3, 2)\n",
    "    True\n",
    "    \n",
    "    We print out the distance matrix of ngrams1 and ngrams2 with different length.\n",
    "    >>> print(distance_matrix(count_ngrams_multi(['abcd', 'abc', 'acde'], 2), \n",
    "    ...                       count_ngrams_multi(['abcd', 'abc'        ], 2)))\n",
    "    [[0.         0.33333333]\n",
    "    [0.33333333 0.        ]\n",
    "    [0.8        1.        ]]\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    # my code is 5 lines\n",
    "    \n",
    "    dis_matrix = np.zeros((len(ngrams1), len(ngrams2))) # We first create an array with zeros fill inside\n",
    "    \n",
    "    for i in range(len(ngrams1)): # we use i and ngrams1 represent row and use j and ngrams2 represents column.\n",
    "        for j in range(len(ngrams2)):\n",
    "            dis_matrix[i][j] = JD(ngrams1[i], ngrams2[j])\n",
    "    \n",
    "    return dis_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6b1d6af8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding tests in NoName\n",
      "Trying:\n",
      "    distance_matrix(count_ngrams_multi(['abcd', 'abc', 'acde'], 2), \n",
      "                    count_ngrams_multi(['abcd', 'abc', 'acde'], 2)).shape == (3, 3)\n",
      "Expecting:\n",
      "    True\n",
      "ok\n",
      "Trying:\n",
      "    distance_matrix(count_ngrams_multi(['abcd', 'abc', 'acde'], 2), \n",
      "                    count_ngrams_multi(['abcd', 'abc', 'acde'], 2)).diagonal()\n",
      "Expecting:\n",
      "    array([0., 0., 0.])\n",
      "ok\n",
      "Trying:\n",
      "    distance_matrix(count_ngrams_multi(['abcd', 'abc', 'acde'], 2), \n",
      "                    count_ngrams_multi(['abcd', 'abc'        ], 2)).shape == (3, 2)\n",
      "Expecting:\n",
      "    True\n",
      "ok\n",
      "Trying:\n",
      "    print(distance_matrix(count_ngrams_multi(['abcd', 'abc', 'acde'], 2), \n",
      "                          count_ngrams_multi(['abcd', 'abc'        ], 2)))\n",
      "Expecting:\n",
      "    [[0.         0.33333333]\n",
      "    [0.33333333 0.        ]\n",
      "    [0.8        1.        ]]\n",
      "ok\n"
     ]
    }
   ],
   "source": [
    "doctest.run_docstring_examples(distance_matrix, globals(), verbose=True, optionflags=doctest.NORMALIZE_WHITESPACE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f8d01a",
   "metadata": {},
   "source": [
    "**Question 6**. We need another helper function which calculates the *cluster score*. \n",
    "It will be similar to the $k$-means objective mentioned in lectures but not identical. We define the \n",
    "cluster score as 1 minus the normalised sum of intra-cluster distances. That is, we \n",
    "take all the points in a particular cluster and sum all the distances between them.\n",
    "Sum that over all clusters. Normalise that by dividing by the sum of distances \n",
    "between *all* points (regardless of cluster). The result is in [0, 1], of course. It's 0 if\n",
    "the points within each cluster are identical. It's larger if not. Finally, take 1\n",
    "minus that, so that higher is better. So 1 is the perfect score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "d120529d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_score(D, labels):\n",
    "    \"\"\"\n",
    "    Calculate a clustering score, defined as the 1 minus the\n",
    "    normalised sum of intra-cluster distances. The result is in\n",
    "    [0, 1], where higher is better.\n",
    "    \n",
    "    Here we see two clusters of two points each and in each cluster, and\n",
    "    the points are at zero distance from each other,\n",
    "    so we get a perfect score of 1.0\n",
    "    >>> cluster_score(np.array([[0, 0, 1, 1], \n",
    "    ...                         [0, 0, 1, 1],\n",
    "    ...                         [1, 1, 0, 0], \n",
    "    ...                         [1, 1, 0, 0]]), \n",
    "    ...               [0, 0, 1, 1])\n",
    "    1.0\n",
    "    \n",
    "    Again, two clusters of two points each, but\n",
    "    this time they are clustered badly! Nearby points are in\n",
    "    different clusters.\n",
    "    >>> cluster_score(np.array([[0, 0, 1, 1], \n",
    "    ...                         [0, 0, 1, 1],\n",
    "    ...                         [1, 1, 0, 0], \n",
    "    ...                         [1, 1, 0, 0]]), \n",
    "    ...               [0, 1, 0, 1])\n",
    "    0.5\n",
    "    \n",
    "    Again, two clusters, but more realistic:\n",
    "    >>> round(cluster_score(np.array([[0, 1, 7, 8], \n",
    "    ...                               [1, 0, 6, 5],\n",
    "    ...                               [7, 6, 0, 1], \n",
    "    ...                               [8, 5, 1, 0]]), \n",
    "    ...                     [0, 0, 1, 1]), 3)\n",
    "    0.929\n",
    "    \n",
    "    Again, two clusters, three points in one cluster and one point in the other cluster.\n",
    "    >>> round(cluster_score(np.array([[0, 0, 1, 1], \n",
    "    ...                               [0, 0, 1, 1],\n",
    "    ...                               [1, 1, 0, 0], \n",
    "    ...                               [1, 1, 0, 0]]), \n",
    "    ...                    [1, 0, 0, 0]), 3)\n",
    "    0.5\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    # my code is 5 lines\n",
    "    \n",
    "    \n",
    "    # The input *D* represents distance matrix, in which each row represents each counter (point) of ngrams1 \n",
    "    # and each column represents each counter (point) of ngrams2.\n",
    "    # The intersection value of each row and column represents the dissimilarity of corresponding counters (points) of ngrams.\n",
    "    \n",
    "    # The input *labels* represents clusters with respect to the conters (points) of the ngrams.\n",
    "    \n",
    "    # In this case, we assume distance matrix D is symmetric.\n",
    "    \n",
    "    # we store the each cluster and corresponding index into a dictionary.\n",
    "    cl_ind_dict = dict()\n",
    "    for i, cluster in enumerate(labels):\n",
    "        if cluster in cl_ind_dict:\n",
    "            cl_ind_dict[cluster].append(i)\n",
    "        else:\n",
    "            cl_ind_dict[cluster] = [i]\n",
    "        \n",
    "    # calculate the sum of distance of the points within each cluster.\n",
    "    dist_cluster = 0.0\n",
    "    for idx, cluster in enumerate(cl_ind_dict): # extract the key which is a cluster from the dictionary.\n",
    "        cl_ind_list = cl_ind_dict[cluster] # store the counter index with the same cluster labels in a list\n",
    "        \n",
    "        for i,v in enumerate(cl_ind_list[:-1]): # extract the counter index from the list\n",
    "            for j in cl_ind_list[i+1:]: # calculate all of the possible distances within each cluster.\n",
    "                dist_cluster += D[v][j] + D[j][v]        \n",
    "                \n",
    "    # calculate the sum of distances of all points\n",
    "    dist_all = np.sum(D)\n",
    "\n",
    "    score = 1 - dist_cluster / dist_all\n",
    "    \n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "2edd78b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding tests in NoName\n",
      "Trying:\n",
      "    cluster_score(np.array([[0, 0, 1, 1], \n",
      "                            [0, 0, 1, 1],\n",
      "                            [1, 1, 0, 0], \n",
      "                            [1, 1, 0, 0]]), \n",
      "                  [0, 0, 1, 1])\n",
      "Expecting:\n",
      "    1.0\n",
      "ok\n",
      "Trying:\n",
      "    cluster_score(np.array([[0, 0, 1, 1], \n",
      "                            [0, 0, 1, 1],\n",
      "                            [1, 1, 0, 0], \n",
      "                            [1, 1, 0, 0]]), \n",
      "                  [0, 1, 0, 1])\n",
      "Expecting:\n",
      "    0.5\n",
      "ok\n",
      "Trying:\n",
      "    round(cluster_score(np.array([[0, 1, 7, 8], \n",
      "                                  [1, 0, 6, 5],\n",
      "                                  [7, 6, 0, 1], \n",
      "                                  [8, 5, 1, 0]]), \n",
      "                        [0, 0, 1, 1]), 3)\n",
      "Expecting:\n",
      "    0.929\n",
      "ok\n",
      "Trying:\n",
      "    round(cluster_score(np.array([[0, 0, 1, 1], \n",
      "                                  [0, 0, 1, 1],\n",
      "                                  [1, 1, 0, 0], \n",
      "                                  [1, 1, 0, 0]]), \n",
      "                       [1, 0, 0, 0]), 3)\n",
      "Expecting:\n",
      "    0.5\n",
      "ok\n"
     ]
    }
   ],
   "source": [
    "doctest.run_docstring_examples(cluster_score, globals(), verbose=True, optionflags=doctest.NORMALIZE_WHITESPACE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14018ce1",
   "metadata": {},
   "source": [
    "**Question 7**. Now, we're ready to put everything together. Create a class `NGramsKClusters` which inherits from `BaseEstimator` and `ClusterMixin`. This is analogous to the nearest-neighbours class we created when studying the Scikit-Learn API.\n",
    "\n",
    "It should have an `__init__` function which stores the values of $n$ and $k$. It should have a `fit` method, where the input `X` is a list of sequences. In `fit` we should precompute the distance matrix.\n",
    "\n",
    "To actually do the clustering in `fit`, we should use the Scikit-Learn `AgglomerativeClustering` method. We should pass it the arguments `linkage=\"average\"` and `affinity=\"precomputed\"`, but we don't need to understand the details here.\n",
    "\n",
    "Inside `fit`, our class should store the $n$-grams `ngrams_`, the distance matrix `distances_`, the labels found by AgglomerativeClustering `labels_`, and the clustering score `score_`. \n",
    "\n",
    "The function `fit_predict` will be supplied by the API. We don't have to write it. Behind the scenes it will call `fit` and immediately return the cluster label for each point in `X`. This is useful for clustering, because typically our only goal is to get cluster labels for the training data, ie we don't later on want to predict with any other `X`.\n",
    "\n",
    "However in our case, we will also write `predict`. The input to this is a new `X` and for each sequence in that `X` we should find the label of the nearest sequence in our training data. This can use our existing `distances_`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "d8810814",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NGramsKClusters(BaseEstimator, ClusterMixin):\n",
    "    \"\"\"\n",
    "    >>> NGramsKClusters(n_clusters=2, ngram_len=3)\n",
    "    NGramsKClusters(n_clusters=2, ngram_len=3)\n",
    "\n",
    "    Don't forget, fit() should return self:\n",
    "    >>> NGramsKClusters(n_clusters=2, ngram_len=3).fit(X)\n",
    "    NGramsKClusters(n_clusters=2, ngram_len=3)\n",
    "    \n",
    "    If we fit with 2 clusters the output should be 0's and 1's\n",
    "    >>> set(NGramsKClusters(n_clusters=2, ngram_len=3).fit_predict(X))\n",
    "    {0, 1}\n",
    "\n",
    "    If we fit with 2 clusters the output should be 0's and 1's and 2's\n",
    "    >>> set(NGramsKClusters(n_clusters=3, ngram_len=3).fit_predict(X))\n",
    "    {0, 1, 2}\n",
    "\n",
    "    After fitting, the object should have various trailing-underscore values:\n",
    "    >>> nk = NGramsKClusters(n_clusters=2, ngram_len=3).fit(X)\n",
    "    >>> all(hasattr(nk, name)\n",
    "    ...     for name in ['ngrams_', 'labels_', 'score_', 'distances_'])\n",
    "    True\n",
    "\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    # my code is ~20 lines\n",
    "    \n",
    "    # It should have an __init__ function which stores the values of n and k. \n",
    "    def __init__(self, n_clusters, ngram_len):\n",
    "        #super().__init__()\n",
    "        \n",
    "        self.n_clusters = n_clusters\n",
    "        self.ngram_len = ngram_len\n",
    "        \n",
    "        self.clustering = AgglomerativeClustering(n_clusters=self.n_clusters, \n",
    "                                                  affinity='precomputed', \n",
    "                                                  linkage=\"average\")\n",
    "        \n",
    "    \n",
    "    # It should have a fit method, where the input X is a list of sequences. \n",
    "    # In fit we should precompute the distance matrix. distance_matrix(ngrams1, ngrams2)\n",
    "    \n",
    "    # Inside fit, our class should store the n-grams *ngrams_*, \n",
    "        # the distance matrix *distances_*, \n",
    "        # the labels found by AgglomerativeClustering *labels_*, \n",
    "        # and the clustering score *score_*.\n",
    "    def fit(self, X):\n",
    "        X_ = X.copy()\n",
    "        \n",
    "        ngrams_ = count_ngrams_multi(X_, self.ngram_len)\n",
    "                \n",
    "        distances_ = distance_matrix(ngrams_, ngrams_)  \n",
    "        \n",
    "        X_ = np.array(X_)\n",
    "        X_.reshape((150,1))\n",
    "        \n",
    "        self.clustering.fit(X_)\n",
    "        \n",
    "        labels = self.clustering.labels_\n",
    "        \n",
    "        score_ = cluster_score(distances_, labels)\n",
    "    \n",
    "    # The input to this is a new X \n",
    "    # and for each sequence in that X we should find the label of the nearest sequence in our training data. \n",
    "    # This can use our existing *distances_*.\n",
    "    def predict(self, X_new):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "2db88c38-a685-4d1a-8f53-abff33a859ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/2t/_ww03y3x43s96hqdk9grd8qc0000gn/T/ipykernel_1541/955268999.py:54: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  X_ = np.array(X_)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;31mTypeError\u001b[0m: float() argument must be a string or a number, not 'list'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [176], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mNGramsKClusters\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_clusters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mngram_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn [175], line 57\u001b[0m, in \u001b[0;36mNGramsKClusters.fit\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     54\u001b[0m X_ \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(X_)\n\u001b[1;32m     55\u001b[0m X_\u001b[38;5;241m.\u001b[39mreshape((\u001b[38;5;241m150\u001b[39m,\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m---> 57\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclustering\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclustering\u001b[38;5;241m.\u001b[39mlabels_\n\u001b[1;32m     61\u001b[0m score_ \u001b[38;5;241m=\u001b[39m cluster_score(distances_, labels)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/research/lib/python3.9/site-packages/sklearn/cluster/_agglomerative.py:914\u001b[0m, in \u001b[0;36mAgglomerativeClustering.fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    896\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    897\u001b[0m     \u001b[38;5;124;03m\"\"\"Fit the hierarchical clustering from features, or distance matrix.\u001b[39;00m\n\u001b[1;32m    898\u001b[0m \n\u001b[1;32m    899\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    912\u001b[0m \u001b[38;5;124;03m        Returns the fitted instance.\u001b[39;00m\n\u001b[1;32m    913\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 914\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mensure_min_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    915\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit(X)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/research/lib/python3.9/site-packages/sklearn/base.py:577\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    575\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValidation should be done on X, y or both.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    576\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m no_val_y:\n\u001b[0;32m--> 577\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mX\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    578\u001b[0m     out \u001b[38;5;241m=\u001b[39m X\n\u001b[1;32m    579\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_y:\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/research/lib/python3.9/site-packages/sklearn/utils/validation.py:856\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m    854\u001b[0m         array \u001b[38;5;241m=\u001b[39m array\u001b[38;5;241m.\u001b[39mastype(dtype, casting\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munsafe\u001b[39m\u001b[38;5;124m\"\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    855\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 856\u001b[0m         array \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    857\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ComplexWarning \u001b[38;5;28;01mas\u001b[39;00m complex_warning:\n\u001b[1;32m    858\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    859\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mComplex data not supported\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(array)\n\u001b[1;32m    860\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcomplex_warning\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence."
     ]
    }
   ],
   "source": [
    "NGramsKClusters(n_clusters=2, ngram_len=3).fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2adb6507",
   "metadata": {},
   "outputs": [],
   "source": [
    "doctest.run_docstring_examples(NGramsKClusters, globals(), verbose=True, optionflags=doctest.NORMALIZE_WHITESPACE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47478e6e",
   "metadata": {},
   "source": [
    "**Question 8**. Now, let's use our class and the data it stores to evaluate.\n",
    "\n",
    "First, write a line of code to create the `NGramsKClusters` object with $n=3$ and $k=3$. Then fit it with `X`. Then print out the score. This doesn't need to be in a function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c65c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# my code is 3 lines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f3b62f3",
   "metadata": {},
   "source": [
    "**Question 9**. Next, here's a query sequence. Which cluster would it go into? Write a line or two of code to answer the question and to show that this result is \"correct\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "968cb5e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = [[0, 2, 1, 0, 1, 2, 0, 1, 2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc7a641",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# my code is ~ 6 lines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d833622a",
   "metadata": {},
   "source": [
    "**Question 10**. Next let's visualise the results. First, use `imshow` to visualise the square distance matrix which has been saved by `fit`. We should get a result like this. It shows that the first 50 in X are all quite similar, and the last 50 in X are all quite similar, and the middle 50 are similar but slightly less so:\n",
    "\n",
    "![\"Title\"](NGramsKClusters_dissimilarity_matrix.png \"alt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c5d5994",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# my code is 1 line"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a19c29b",
   "metadata": {},
   "source": [
    "**Question 11**. Next, let's use $t$-SNE to visualise the data in 2D, with the labels shown as colour. This is similar to what we did under *Representation Learning*. Remember that we have `precomputed` distances. We should get an image something like this (it might be rotated or slightly different in other ways). It shows the three clusters clearly.\n",
    "\n",
    "![Image](NGramsKClusters_embedding.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e2ef98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# my code is 4 lines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b13ee5",
   "metadata": {},
   "source": [
    "**Question 12**. Finally, let's check out how the score changes for different $k$. Again, this is similar to something we did under *Representation Learning*. We should end up with an image something like this:\n",
    "\n",
    "![Image](NGramsKClusters_cluster_score_by_k.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a42a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# my code is ~ 5 lines"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
